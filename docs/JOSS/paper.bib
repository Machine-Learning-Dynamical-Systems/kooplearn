@inproceedings{kostic2024generator,
  author = {Kostic, Vladimir R. and Lounici, Karim and Halconruy, H\'{e}l\`{e}ne and Devergne, Timoth\'{e}e and Pontil, Massimiliano},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.52202/079017-4377},
  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages = {137806--137846},
  publisher = {Curran Associates, Inc.},
  title = {Learning the Infinitesimal Generator of Stochastic Diffusion Processes},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f930c6e1292a1160189a0734f22b465b-Paper-Conference.pdf},
  volume = {37},
  year = {2024}
}

@inproceedings{devergne2024biased,
  author = {Devergne, Timoth\'{e}e and Kostic, Vladimir R. and Parrinello, Michele and Pontil, Massimiliano},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.52202/079017-2404},
  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages = {75495--75521},
  publisher = {Curran Associates, Inc.},
  title = {From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/89edef87915d31de3437b6b2ac5f79e7-Paper-Conference.pdf},
  volume = {37},
  year = {2024}
}

@article{devergne2025slow,
  author = {Devergne, Timoth{\'e}e and Kostic, Vladimir R. and Pontil, Massimiliano and Parrinello, Michele},
  title = {Slow dynamical modes from static averages},
  journal = {The Journal of Chemical Physics},
  volume = {162},
  number = {12},
  pages = {124108},
  year = {2025},
  month = {03},
  abstract = {In recent times, efforts have been made to describe the evolution of a complex system not through long trajectories but via the study of probability distribution evolution. This more collective approach can be made practical using the transfer operator formalism and its associated dynamics generator. Here, we reformulate in a more transparent way the result of Devergne et al. [Adv. Neural Inform. Process. Syst. 37, 75495–75521 (2024)] and show that the lowest eigenfunctions and eigenvalues of the dynamics generator can be efficiently computed using data easily obtainable from biased simulations. We also show explicitly that the long time dynamics can be reconstructed by using the spectral decomposition of the dynamics operator.},
  issn = {0021-9606},
  doi = {10.1063/5.0246248},
  url = {https://doi.org/10.1063/5.0246248},
  eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/5.0246248/20452849/124108_1_5.0246248.pdf},
}

@article{sklearn,
  author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
  title = {Scikit-learn: Machine Learning in Python},
  year = {2011},
  issue_date = {2/1/2011},
  publisher = {JMLR.org},
  volume = {12},
  number = {null},
  issn = {1532-4435},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  journal = {J. Mach. Learn. Res.},
  month = nov,
  pages = {2825–2830},
  numpages = {6}
}

@article{schmid2010,
  title={Dynamic mode decomposition of numerical and experimental data},
  volume={656},
  DOI={10.1017/S0022112010001217},
  journal={Journal of Fluid Mechanics},
  author={Schimid, Peter J.},
  year={2010},
  pages={5–28}
  } 

@article{Molgedey1994,
  title = {Separation of a mixture of independent signals using time delayed correlations},
  volume = {72},
  ISSN = {0031-9007},
  url = {http://dx.doi.org/10.1103/PhysRevLett.72.3634},
  DOI = {10.1103/physrevlett.72.3634},
  number = {23},
  journal = {Physical Review Letters},
  publisher = {American Physical Society (APS)},
  author = {Molgedey,  L. and Schuster,  H. G.},
  year = {1994},
  month = jun,
  pages = {3634–3637}
}

@inbook{Steinwart2008,
  author = {Ingo Steinwart and Andreas Christmann},
  title = {Support Vector Machines},
  booktitle = {Least Squares Support Vector Machines},
  chapter = {},
  pages = {29-70},
  doi = {10.1142/9789812776655_0002},
  URL = {https://www.worldscientific.com/doi/abs/10.1142/9789812776655_0002},
  eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9789812776655_0002},
  abstract = { Abstract The following sections are included: Maximal margin classification and linear SVMs Margin Linear SVM classifier: separable case Linear SVM classifier: non-separable case Kernel trick and Mercer condition Nonlinear SVM classifiers VC theory and structural risk minimization Empirical risk versus generalization error Structural risk minimization SVMs for function estimation SVM for linear function estimation SVM for nonlinear function estimation VC bound on generalization error Modifications and extensions Kernels Extension to other convex cost functions Algorithms Parametric versus non-parametric approaches },
  city = {New York, NY},
  publisher = {Springer New York},
  title = {Support Vector Machines},
  year = {2008},
}

@article{klus2018data,
  author  = {Klus, Stefan and N{\"u}ske, Feliks and Koltai, P{\'e}ter and Wu, Hao and Kevrekidis, Ioannis and Sch{\"u}tte, Christof and No{\'e}, Frank},
  title   = {Data-Driven Model Reduction and Transfer Operator Approximation},
  journal = {Journal of Nonlinear Science},
  year    = {2018},
  volume  = {28},
  number  = {3},
  pages   = {985--1010},
  month   = {jun},
  issn    = {1432-1467},
  doi     = {10.1007/s00332-017-9437-7},
  url     = {https://doi.org/10.1007/s00332-017-9437-7}
}

@InProceedings{schutte2001transfer,
  author="Sch{\"u}tte, Ch.
  and Huisinga, W.
  and Deuflhard, P.",
  editor="Fiedler, Bernold",
  title="Transfer Operator Approach to Conformational Dynamics in Biomolecular Systems",
  booktitle="Ergodic Theory, Analysis, and Efficient Simulation of Dynamical Systems",
  year="2001",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="191--223",
  abstract="The article surveys the development of novel mathematical concepts and algorithmic approaches based thereon in view of their possible applicability to biomolecular design. Both a first deterministic approach, based on the Frobenius-Perron operator corresponding to the flow of the Hamiltonian dynamics, and later stochastic approaches, based on a spatial Markov operator or on Langevin dynamics, can be subsumed under the unified mathematical roof of the transfer operator approach to effective dynamics of molecular systems. The key idea of constructing specific transfer operators especially taylored for the purpose of conformational dynamics appears as the red line throughout the paper. Different steps of the algorithm are exemplified by a trinucleotide molecular system as a small representative of possible RNA drug molecules.",
  isbn="978-3-642-56589-2"
}

@book{kutz2016,
  author = {Kutz, J. Nathan and Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L.},
  title = {Dynamic Mode Decomposition},
  publisher = {Society for Industrial and Applied Mathematics},
  year = {2016},
  doi = {10.1137/1.9781611974508},
  address = {Philadelphia, PA},
  edition   = {},
  URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974508},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974508}
}

@article{williams2015_kdmd,
  title = {A kernel-based method for data-driven koopman spectral analysis},
  journal = {Journal of Computational Dynamics},
  volume = {2},
  number = {2},
  pages = {247-265},
  year = {2015},
  issn = {2158-2491},
  doi = {10.3934/jcd.2015005},
  url = {https://www.aimsciences.org/article/id/ce535396-f8fe-4aa1-b6de-baaf986f6193},
  author = {Matthew O.  Williams and Clarence W. Rowley and Ioannis G.  Kevrekidis},
  keywords = {Koopman operator, dynamic mode decomposition, kernel
    methods, time series analysis, machine learning}
}

@inproceedings{kawahara2016,
  author = {Kawahara, Yoshinobu},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/1728efbda81692282ba642aafd57be3a-Paper.pdf},
  volume = {29},
  year = {2016}
}

@inproceedings{takeishi2017,
  author = {Takeishi, Naoya and Kawahara, Yoshinobu and Yairi, Takehisa},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3a835d3215755c435ef4fe9965a3f2a0-Paper.pdf},
  volume = {30},
  year = {2017}
}

@article{lusch2018,
  doi = {10.1038/s41467-018-07210-0},
  url = {https://doi.org/10.1038/s41467-018-07210-0},
  year = {2018},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {9},
  number = {1},
  author = {Bethany Lusch and J. Nathan Kutz and Steven L. Brunton},
  title = {Deep learning for universal linear embeddings of nonlinear dynamics},
  journal = {Nature Communications}
}

@article{otto2019,
  author = {Otto, Samuel E. and Rowley, Clarence W.},
  title = {Linearly Recurrent Autoencoder Networks for Learning Dynamics},
  journal = {SIAM Journal on Applied Dynamical Systems},
  volume = {18},
  number = {1},
  pages = {558-593},
  year = {2019},
  doi = {10.1137/18M1177846},
  URL = {https://doi.org/10.1137/18M1177846},
  eprint = {https://doi.org/10.1137/18M1177846},
  abstract = {This paper describes a method for learning low-dimensional approximations of nonlinear dynamical systems, based on neural network approximations of the underlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD) provides a useful data-driven approximation of the Koopman operator for analyzing dynamical systems. This paper addresses a fundamental problem associated with EDMD: a trade-off between representational capacity of the dictionary and overfitting due to insufficient data. A new neural network architecture combining an autoencoder with linear recurrent dynamics in the encoded state is used to learn a low-dimensional and highly informative Koopman-invariant subspace of observables. A method is also presented for balanced model reduction of overspecified EDMD systems in feature space. Nonlinear reconstruction using partially linear multikernel regression aims to improve reconstruction accuracy from the low-dimensional state when the data has complex but intrinsically low-dimensional structure. The techniques demonstrate the ability to identify Koopman eigenfunctions of the unforced Duffing equation, create accurate low-dimensional models of an unstable cylinder wake flow, and make short-time predictions of the chaotic Kuramoto--Sivashinsky equation. }
}

@article{pydmd,
  author  = {Sara M. Ichinaga and Francesco Andreuzzi and Nicola Demo and Marco Tezzele and Karl Lapo and Gianluigi Rozza and Steven L. Brunton and J. Nathan Kutz},
  title   = {PyDMD: A Python Package for Robust Dynamic Mode Decomposition},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {417},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v25/24-0739.html}
}

@article{pykoopman, 
  doi = {10.21105/joss.05881}, 
  url = {https://doi.org/10.21105/joss.05881}, 
  year = {2024}, 
  publisher = {The Open Journal}, 
  volume = {9}, 
  number = {94}, 
  pages = {5881}, 
  author = {Pan, Shaowu and Kaiser, Eurika and de Silva, Brian M. and Kutz, J. Nathan and Brunton, Steven L.}, 
  title = {PyKoopman: A Python Package for Data-Driven Approximation of the Koopman Operator}, 
  journal = {Journal of Open Source Software} 
}

@article{pykoop, 
  doi = {10.21105/joss.07947}, 
  url = {https://doi.org/10.21105/joss.07947}, 
  year = {2025}, 
  publisher = {The Open Journal}, 
  volume = {10}, 
  number = {114}, 
  pages = {7947}, 
  author = {Dahdah, Steven and Forbes, James Richard}, 
  title = {pykoop: a Python Library for Koopman Operator Approximation}, 
  journal = {Journal of Open Source Software} 
}

@inproceedings{dlkoopman,
  title={DLKoopman: A deep learning software package for Koopman theory},
  author={Dey, Sourya and Davis, Eric William},
  booktitle={Learning for Dynamics and Control Conference},
  pages={1467--1479},
  year={2023},
  organization={PMLR}
}

@InProceedings{dlkoopman,
  title = 	 {DLKoopman: A deep learning software package for Koopman theory},
  author =       {Dey, Sourya and Davis, Eric William},
  booktitle = 	 {Proceedings of The 5th Annual Learning for Dynamics and Control Conference},
  pages = 	 {1467--1479},
  year = 	 {2023},
  editor = 	 {Matni, Nikolai and Morari, Manfred and Pappas, George J.},
  volume = 	 {211},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--16 Jun},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v211/dey23a/dey23a.pdf},
  url = 	 {https://proceedings.mlr.press/v211/dey23a.html},
  abstract = 	 {We present DLKoopman – a software package for Koopman theory that uses deep learning to learn an encoding of a nonlinear dynamical system into a linear space, while simultaneously learning the linear dynamics. While several previous efforts have either restricted the ability to learn encodings, or been bespoke efforts designed for specific systems, DLKoopman is a generalized tool that can be applied to data-driven learning and analysis of any dynamical system. It can either be trained on data from individual states (snapshots) of a system and used to predict its unknown states, or trained on data from trajectories of a system and used to predict unknown trajectories for new initial states. DLKoopman is available on the Python Package Index (PyPI) as ’dlkoopman’, and includes extensive documentation and tutorials. Additional contributions of the package include a novel metric called Average Normalized Absolute Error for evaluating performance, and a ready-to-use hyperparameter search module for improving performance.}
}

@article{koopmanlab,
  author = {Xiong, Wei and Ma, Muyuan and Huang, Xiaomeng and Zhang, Ziyang and Sun, Pei and Tian, Yang},
  title = {KoopmanLab: Machine learning for solving complex physics equations},
  journal = {APL Machine Learning},
  volume = {1},
  number = {3},
  pages = {036110},
  year = {2023},
  month = {09},
  abstract = {Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator (KNO) family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the KNO, a kind of mesh-independent neural-network-based PDE solvers developed following the dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes, while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high-dimensional, and non-linear PDEs. All variants are validated by mesh-independent and long-term prediction experiments implemented on representative PDEs (e.g., the Navier–Stokes equation and the Bateman–Burgers equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution global-scale climate datasets in earth physics). These demonstrations suggest the potential of KoopmanLab to be a fundamental tool in diverse physics studies related to equations or dynamic systems.},
  issn = {2770-9019},
  doi = {10.1063/5.0157763},
  url = {https://doi.org/10.1063/5.0157763},
  eprint = {https://pubs.aip.org/aip/aml/article-pdf/doi/10.1063/5.0157763/18121014/036110_1_5.0157763.pdf},
}

@inproceedings{pytorch,
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume = {32},
  year = {2019}
}

@software{jax,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}

@inproceedings{kostic2022,
  author = {Kostic, Vladimir R. and Novelli, Pietro and Maurer, Andreas and Ciliberto, Carlo and Rosasco, Lorenzo and Pontil, Massimiliano},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages = {4017--4031},
  publisher = {Curran Associates, Inc.},
  title = {Learning Dynamical Systems via Koopman Operator Regression in Reproducing Kernel Hilbert Spaces},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/196c4e02b7464c554f0f5646af5d502e-Paper-Conference.pdf},
  volume = {35},
  year = {2022}
}

@inproceedings{kostic2023,
  author = {Kostic, Vladimir R. and Lounici, Karim and Novelli, Pietro and Pontil, Massimiliano},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages = {32328--32339},
  publisher = {Curran Associates, Inc.},
  title = {Sharp Spectral Rates for Koopman Operator Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/663bce02a0050c4a11f1eb8a7f1429d3-Paper-Conference.pdf},
  volume = {36},
  year = {2023}
}

@inproceedings{meanti2023,
  author = {Meanti, Giacomo and Chatalic, Antoine and Kostic, Vladimir R. and Novelli, Pietro and Pontil, Massimiliano and Rosasco, Lorenzo},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages = {77242--77276},
  publisher = {Curran Associates, Inc.},
  title = {Estimating Koopman operators with sketching to provably learn large scale dynamical systems},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f3d1e34a15c0af0954ae36a7f811c754-Paper-Conference.pdf},
  volume = {36},
  year = {2023}
}

@article{turri2025randomized,
  author = {Turri, Giacomo and Kostic, Vladimir R. and Novelli, Pietro and Pontil, Massimiliano},
  title = {A Randomized Algorithm to Solve Reduced Rank Operator Regression},
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {8},
  number = {1},
  pages = {23-45},
  year = {2026},
  doi = {10.1137/24M1632097},
  URL = {https://doi.org/10.1137/24M1632097},
  eprint = {https://doi.org/10.1137/24M1632097}
}

@article{mardt2018,
  doi = {10.1038/s41467-017-02388-1},
  url = {https://doi.org/10.1038/s41467-017-02388-1},
  year = {2018},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {9},
  number = {1},
  author = {Andreas Mardt and Luca Pasquali and Hao Wu and Frank No{\'{e}}},
  title = {{VAMPnets} for deep learning of molecular kinetics},
  journal = {Nature Communications}
}

@misc{turri2025self,
  title={Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems}, 
  author={Giacomo Turri and Luigi Bonati and Kai Zhu and Massimiliano Pontil and Pietro Novelli},
  year={2025},
  eprint={2505.18671},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2505.18671}, 
}

@inproceedings{bevanda2023,
  author = {Bevanda, Petar and Beier, Max and Lederer, Armin and Sosnowski, Stefan and H\"{u}llermeier, Eyke and Hirche, Sandra},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages = {16207--16221},
  publisher = {Curran Associates, Inc.},
  title = {Koopman Kernel Regression},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/34678d08b36076de986df95c5bbba92f-Paper-Conference.pdf},
  volume = {36},
  year = {2023}
}

@inproceedings{kostic2024neural,
  author = {Kostic, Vladimir R. and Lounici, Karim and Pacreau, Gr\'{e}goire and Turri, Giacomo and Novelli, Pietro and Pontil, Massimiliano},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.52202/079017-1950},
  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
  pages = {60999--61039},
  publisher = {Curran Associates, Inc.},
  title = {Neural Conditional Probability for Uncertainty Quantification},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/705b97ecb07ae86524d438abac97a3e2-Paper-Conference.pdf},
  volume = {37},
  year = {2024}
}

@InProceedings{kostic2024consistent,
  title = 	 {Consistent Long-Term Forecasting of Ergodic Dynamical Systems},
  author =       {Kostic, Vladimir R. and Lounici, Karim and Inzerilli, Prune and Novelli, Pietro and Pontil, Massimiliano},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {25370--25395},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/kostic24a/kostic24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/kostic24a.html},
  abstract = 	 {We study the problem of forecasting the evolution of a function of the state (observable) of a discrete ergodic dynamical system over multiple time steps. The elegant theory of Koopman and transfer operators can be used to evolve any such function forward in time. However, their estimators are usually unreliable in long-term forecasting. We show how classical techniques of eigenvalue deflation from operator theory and feature centering from statistics can be exploited to enhance standard estimators. We develop a novel technique to derive high probability bounds on powers of empirical estimators. Our approach, rooted in the stability <em>theory of non-normal operators</em>, allows us to establish uniform in time bounds for the forecasting error, which hold even on <em>infinite time horizons</em>. We further show that our approach can be seamlessly employed to forecast future state distributions from an initial one, with provably uniform error bounds. Numerical experiments illustrate the advantages of our approach in practice.}
}

@inproceedings{kostic2024learning,
  author = {Kostic, Vladimir R. and Novelli, Pietro and Grazzi, Riccardo and Lounici, Karim and pontil, massimiliano},
  booktitle = {International Conference on Representation Learning},
  editor = {B. Kim and Y. Yue and S. Chaudhuri and K. Fragkiadaki and M. Khan and Y. Sun},
  pages = {54329--54341},
  title = {Learning invariant representations of time-homogeneous stochastic dynamical systems},
  url = {https://proceedings.iclr.cc/paper_files/paper/2024/file/eeab2e00835c71d64458ad1821e05664-Paper-Conference.pdf},
  volume = {2024},
  year = {2024}
}

@InProceedings{bevanda2025,
  title = 	 {Koopman-Equivariant Gaussian Processes},
  author =       {Bevanda, Petar and Beier, Max and Capone, Alexandre and Sosnowski, Stefan Georg and Hirche, Sandra and Lederer, Armin},
  booktitle = 	 {Proceedings of The 28th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3151--3159},
  year = 	 {2025},
  editor = 	 {Li, Yingzhen and Mandt, Stephan and Agrawal, Shipra and Khan, Emtiyaz},
  volume = 	 {258},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--05 May},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v258/main/assets/bevanda25a/bevanda25a.pdf},
  url = 	 {https://proceedings.mlr.press/v258/bevanda25a.html},
  abstract = 	 {We propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and representational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance – which we refer to as Koopman equivariance – we obtain a  GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems.}
}

@article{koopman1931,
  author = {B. O. Koopman },
  title = {Hamiltonian Systems and Transformation in Hilbert Space},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {17},
  number = {5},
  pages = {315-318},
  year = {1931},
  doi = {10.1073/pnas.17.5.315},
  URL = {https://www.pnas.org/doi/abs/10.1073/pnas.17.5.315},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.17.5.315}
  }

@book{applebaum2009,
  title = {Lévy Processes and Stochastic Calculus},
  ISBN = {9780511809781},
  url = {http://dx.doi.org/10.1017/CBO9780511809781},
  DOI = {10.1017/cbo9780511809781},
  publisher = {Cambridge University Press},
  author = {Applebaum,  David},
  year = {2009},
  month = apr 
}

@article{mezic2005,
  author  = {Mezi{\'c}, Igor},
  title   = {Spectral Properties of Dynamical Systems, Model Reduction and Decompositions},
  journal = {Nonlinear Dynamics},
  year    = {2005},
  volume  = {41},
  number  = {1},
  pages   = {309--325},
  month   = {aug},
  issn    = {1573-269X},
  doi     = {10.1007/s11071-005-2824-x},
  url     = {https://doi.org/10.1007/s11071-005-2824-x}
}

@article{ostruszka2000dynamical,
  title = {Dynamical entropy for systems with stochastic perturbation},
  author = {Ostruszka, Andrzej and Pako\ifmmode \acute{n}\else \'{n}\fi{}ski, Prot and S\l{}omczy\ifmmode \acute{n}\else \'{n}\fi{}ski, Wojciech and \ifmmode \dot{Z}\else \.{Z}\fi{}yczkowski, Karol},
  journal = {Phys. Rev. E},
  volume = {62},
  issue = {2},
  pages = {2018--2029},
  numpages = {0},
  year = {2000},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.62.2018},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.62.2018}
}

@article{Prinz2011,
  title = {Markov models of molecular kinetics: Generation and validation},
  volume = {134},
  ISSN = {1089-7690},
  url = {http://dx.doi.org/10.1063/1.3565032},
  DOI = {10.1063/1.3565032},
  number = {17},
  journal = {The Journal of Chemical Physics},
  publisher = {AIP Publishing},
  author = {Prinz,  Jan-Hendrik and Wu,  Hao and Sarich,  Marco and Keller,  Bettina and Senne,  Martin and Held,  Martin and Chodera,  John D. and Sch\"{u}tte,  Christof and Noé,  Frank},
  year = {2011},
  month = may 
}

@ARTICLE{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}