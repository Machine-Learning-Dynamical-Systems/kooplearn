{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo-cp5L6mQnh"
   },
   "source": [
    "# Logistic Regression Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwxnFyciguT"
   },
   "source": [
    "> _Authors:_ Erfan Mirzaei — [erfunmirzaei](https://github.com/erfunmirzaei) & Giacomo Turri — [g-turri](https://github.com/g-turri)\n",
    "\n",
    "In this notebook, our objective is to replicate the **Logistic Map** experiment presented in the paper {footcite:t}`Kostic2023DPNets`, using the `kooplearn` library. This experiment explores the complexities of learning a representation of the noisy logistic map, a one-dimensional dynamical system characterized by:\n",
    "\n",
    "$$\n",
    "    x_{t + 1} = (rx_{t}(1 - x_{t}) + \\xi_{t}) \\mod 1.\n",
    "$$\n",
    "\n",
    "Here $\\xi_{t}$ is an i.i.d trigonometric noise term with density $\\propto \\cos^{N}(x)$ with $N$ being an even integer, and $r$ is a positive number characterizing the logistic map. In this experiment, $r$ is set to 4 for which we can derive the exact solution.\n",
    "\n",
    "In particular, the transfer operator for this problem has rank $N + 1$, it is non-normal, and its eigenvalues and eigenfunctions can be computed exactly. For more information about the transfer operator properties of this dynamical system, see {footcite:t}`Kostic2022`.\n",
    "\n",
    "It is noteworthy that because the associated transfer operator, $\\mathcal{T}$, is *non-normal*, learning its spectral decomposition is challenging {footcite:t}`Kostic2023SpectralRates`. Therefore, this experiment would be a good choice to evaluate the performance of learned representation.\n",
    "Especially since we know $\\mathcal{T}$ can be computed exactly here we sidestep the problem of operator regression and just focus on evaluating the quality of the learned representation.\n",
    "\n",
    "In this regard, we first check whether the `kooplearn` is installed or not, and if not install it.\n",
    "## Setup `kooplearn`+ Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T14:59:24.936367Z",
     "start_time": "2024-03-18T14:59:22.341698Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135487,
     "status": "ok",
     "timestamp": 1710415180901,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "TGJJ1LyAPard",
    "outputId": "6d055677-1629-4606-b527-725eba4ede9b"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "for module in ['kooplearn']:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "    except ImportError:\n",
    "      if module == 'kooplearn':\n",
    "        module = 'kooplearn[full]'\n",
    "      %pip install -q {module}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T14:59:30.502885Z",
     "start_time": "2024-03-18T14:59:30.499996Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710420134492,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "bKH7k0uaq9a7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T14:59:34.114198Z",
     "start_time": "2024-03-18T14:59:34.111721Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1710415185484,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3mf3g0tYtZ5E"
   },
   "outputs": [],
   "source": [
    "# All hyperparameters\n",
    "# {N: 20\n",
    "# num_rng_seeds: 20\n",
    "# num_train: 16385 #batch_size*4 + 1\n",
    "# num_val: 10000\n",
    "# num_test: 10000\n",
    "# feature_dim: 8\n",
    "# batch_size: 8192\n",
    "# layer_dims: [64, 128, 64]\n",
    "# max_epochs: 500\n",
    "# trial_budget: 50\n",
    "# learning_rate: 5e-5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrpKkTtV7AeO"
   },
   "source": [
    "## Building the Dataset\n",
    "\n",
    "Since in this experiment we want to learn the logistic map, we should build our dataset first. To this purpose, we follow these steps respectively:\n",
    "\n",
    "- Define our dynamical system;\n",
    "- Sample from this system to generate the training, validation, and test trajectories;\n",
    "- Use {meth}`kooplearn.data.traj_to_contexts` to transform the trajectories in context windows (see [Kooplearn’s data paradigm](kooplearn_data_paradigm) for more information);\n",
    "- Create a DataLoader to use PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RmAgLZuXQ7K"
   },
   "source": [
    "### Defining the Noisy Logistic Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfXJyy-4FP4h"
   },
   "source": [
    "Before defining the dynamical system, we should determine the number of training, validation, and test samples. As we intended to reproduce the paper's result we set the training number and batch size as they reported in the appendix `F.1`. Here, as we know the analytical solution of the problem, we set the number of test samples arbitrarily.  However, we will not use these samples for evaluation of the learned representation in this problem. Also, we will not perform hyperparameter tuning and just set the hyperparameters given in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:08:14.693614Z",
     "start_time": "2024-03-18T15:08:14.690444Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710415185484,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "6CnbIHbOq2p1"
   },
   "outputs": [],
   "source": [
    "# Defining the number of samples for each data split\n",
    "batch_size = 8192\n",
    "n_train_samples = 16385 #batch_size*2 + 1\n",
    "n_val_samples = 10000\n",
    "n_test_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9cWvQshG1QY"
   },
   "source": [
    "In the `kooplearn` library, you can easily import {class}`kooplearn.datasets.LogisticMap` and generate the samples. For defining the dynamical system we can instantiate an object from the class, {class}`kooplearn.datasets.LogisticMap`, which takes $r$, $N$, and a random seed just for the sake of reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:10:22.603462Z",
     "start_time": "2024-03-18T15:10:19.072761Z"
    },
    "executionInfo": {
     "elapsed": 14645,
     "status": "ok",
     "timestamp": 1710415200125,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3jEk8LTTEHiE"
   },
   "outputs": [],
   "source": [
    "from kooplearn.datasets import LogisticMap\n",
    "\n",
    "# Model Initialization\n",
    "N_exp = 20 # Exponent of the trigonometric noise\n",
    "logmap = LogisticMap(N = N_exp, rng_seed = 0) # Setting the rng_seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcRnD3hbYifO"
   },
   "source": [
    "### Generating training, validation, and test trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuAR2qwLK1Er"
   },
   "source": [
    "After instantiating from {class}`kooplearn.datasets.LogisticMap`, we can use this object to generate the samples calling the `sample` method. This function takes as inputs the initial condition/point, *$X_0$*, and the number of samples, *num_samples*, and returns the sampled trajectory starting from *$X_0$* and continuing for the number of samples. Thus, in total, it has the length *num_samples + 1*. After sampling our trajectory, we can split it to the training, validation and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:13:25.123473Z",
     "start_time": "2024-03-18T15:13:24.130806Z"
    },
    "executionInfo": {
     "elapsed": 2692,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "T24yNv38uRac"
   },
   "outputs": [],
   "source": [
    "# Data pipeline\n",
    "sample_traj = logmap.sample(0.5, n_train_samples + n_val_samples + n_test_samples) #initalization point, X0 = 0.5 and T = length of trajectory.\n",
    "\n",
    "dataset = {\n",
    "    \"train\": sample_traj[: n_train_samples],\n",
    "    \"validation\": sample_traj[n_train_samples : n_train_samples + n_val_samples],\n",
    "    \"test\": sample_traj[n_train_samples + n_val_samples :],\n",
    "}\n",
    "\n",
    "train_data = torch.from_numpy(dataset[\"train\"]).float()\n",
    "val_data = torch.from_numpy(dataset[\"validation\"]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUd4cjkxjkVq"
   },
   "source": [
    "### From trajectories to context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vcREDehPq-h"
   },
   "source": [
    "As explained in [the documentation page of kooplearn's data paradigm](kooplearn_data_paradigm), `kooplearn` expects data organized into context windows. Broadly speaking, a context window is a (usually short) sequence of consecutive observations of the system. A context window encloses the 'past' in its _lookback window_ and the 'future' in its _lookforward window_.\n",
    "\n",
    "Kooplearn provides simple utility functions to create context windows out of trajectories, such as {meth}`kooplearn.data.traj_to_contexts`. By default, {meth}`kooplearn.data.traj_to_contexts` will create contexts of length 2, as the minimal amount of frames to learn a dynamical relation is at least one for both the lookback and lookforward windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:23:01.515672Z",
     "start_time": "2024-03-18T15:23:01.510442Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "JIhffMQHxam6"
   },
   "outputs": [],
   "source": [
    "from kooplearn.data import traj_to_contexts\n",
    "\n",
    "contexts = {k: traj_to_contexts(v, backend='torch') for k, v in dataset.items()} # Converting the trajectories to contexts\n",
    "for split, ds in contexts.items():\n",
    "    print(f\"{split.capitalize()} contexts have shape {ds.shape}: {len(ds)} contexts of length {ds.context_length} with {ds.shape[2]} features each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6-Liuqj6Se"
   },
   "source": [
    "### Build Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKB9S5FukFsG"
   },
   "source": [
    "Here, we can simply use `Dataloader` function of PyTorch and pass the data and batch size as inputs.\n",
    "\n",
    "N.B.: In order for the dataloaders to work properly with the context window datasets, we need to create the dataloader by passing the {meth}`kooplearn.nn.data.collate_context_dataset` to the dataloader as `DataLoader(..., collate_fn=collate_context_dataset)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:32:15.804528Z",
     "start_time": "2024-03-18T15:32:15.801159Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "8SdSbmUMj_mo"
   },
   "outputs": [],
   "source": [
    "from kooplearn.nn.data import collate_context_dataset\n",
    "\n",
    "train_dl = DataLoader(contexts['train'], batch_size= batch_size, shuffle=True, collate_fn=collate_context_dataset)\n",
    "val_dl = DataLoader(contexts['validation'], batch_size=len(contexts['validation']), shuffle=False, collate_fn=collate_context_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWnw7Ie87pDv"
   },
   "source": [
    "## DPNets Training Algorithm\n",
    "\n",
    "To run the DPNets training algorithm, we should give DNNs, optimizer, number of training steps, and metric loss parameter as inputs of algorithm. Thus, here first we set the hyperparameters as reported in the original paper. And in the following parts describe:\n",
    "\n",
    "- Sinusoidal Embedding\n",
    "- Building NN Using PyTorch\n",
    "- Build DPNet with Kaiming initialization\n",
    "- (Hyperparameter Optimization for finding best learning rate with lightning Library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:08.540941Z",
     "start_time": "2024-03-18T16:04:08.537394Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "AbGr1CoGVALW"
   },
   "outputs": [],
   "source": [
    "# Init report dict\n",
    "dl_reports = {}\n",
    "\n",
    "opt = torch.optim.Adam\n",
    "max_epochs = 100\n",
    "num_rng_seeds = 1\n",
    "layer_dims = [64, 128, 64]\n",
    "metric_deformation_coeff = 1\n",
    "feature_dim = 7\n",
    "\n",
    "trainer_kwargs = {\n",
    "    \"accelerator\": \"cpu\",\n",
    "    \"devices\": 1,\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"log_every_n_steps\": 3,\n",
    "    \"enable_model_summary\": False,\n",
    "    \"enable_progress_bar\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyCK-fl7oScW"
   },
   "source": [
    "### Sinusoidal Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC2iKn1wSfS8"
   },
   "source": [
    "The logistic map is defined only in the interval [0, 1], and crucially is periodic. Therefore, featurizing the signal with trigonometric functions constructs models which account for this periodicity by design. We simply considering sin(2$\\mathcal{\\pi}$x) and cos(2$\\mathcal{\\pi}$x) as embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:09.822660Z",
     "start_time": "2024-03-18T16:04:09.819327Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "MKGNFXipHghr"
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is in [0, 1]\n",
    "        x = 2 * torch.pi * x\n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhVkavlKobXE"
   },
   "source": [
    "### Creting MLP using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmtoJDvsVLgw"
   },
   "source": [
    "Now we use PyTorch library to define our neural networks with the specified hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:10.495591Z",
     "start_time": "2024-03-18T16:04:10.490314Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "Iw9ZvcnjGnR2"
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim: int, layer_dims: list[int], activation=torch.nn.LeakyReLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        lin_dims = ([2] + layer_dims + [feature_dim])  # The 2 is for the sinusoidal embedding\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for layer_idx in range(len(lin_dims) - 2):\n",
    "            layers.append(torch.nn.Linear(lin_dims[layer_idx], lin_dims[layer_idx + 1], bias=False))\n",
    "            layers.append(activation())\n",
    "\n",
    "        layers.append(torch.nn.Linear(lin_dims[-2], lin_dims[-1], bias=True))\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.sin_embedding = SinusoidalEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sinusoidal embedding\n",
    "        x = self.sin_embedding(x).float()\n",
    "        # MLP\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OACmEQzGp2Wp"
   },
   "source": [
    "### Building DPNet with Kaiming Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1jYIq57uMBS"
   },
   "source": [
    "Now we can define the DPNet using {class}`kooplearn.models.feature_maps.DPNet`. It takes both NNs(one for $\\mathcal{Ψ_w}$ and one for$\\mathcal{Ψ'_w}$), optimizer, trainer(Which is an initialized `Lightning` trainer), the metric deformation loss coefficient, a boolean variable to see wheter to compute the VAMP score with centered covariances and seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:11.174576Z",
     "start_time": "2024-03-18T16:04:11.152390Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1710415203689,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "LNS2W18mzYQh",
    "outputId": "5cb5be9f-ab52-40b6-9c24-d59fb49d28d3"
   },
   "outputs": [],
   "source": [
    "from kooplearn.models.feature_maps.nn import NNFeatureMap\n",
    "from kooplearn.nn import DPLoss\n",
    "import lightning\n",
    "\n",
    "trainer = lightning.Trainer(**trainer_kwargs)\n",
    "net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "DPNets = {'loss_fn': DPLoss, 'loss_kwargs': {'relaxed': False, 'metric_deformation': metric_deformation_coeff, 'center_covariances': False}}\n",
    "seed = 42\n",
    "\n",
    "# Defining the model\n",
    "dpnet_fmap = NNFeatureMap(\n",
    "    SimpleMLP,\n",
    "    DPNets['loss_fn'],\n",
    "    opt,\n",
    "    trainer,\n",
    "    encoder_kwargs = net_kwargs,\n",
    "    loss_kwargs = DPNets['loss_kwargs'],\n",
    "    optimizer_kwargs = {\"lr\": 5e-5},\n",
    "    lagged_encoder = SimpleMLP,\n",
    "    lagged_encoder_kwargs = net_kwargs,\n",
    "    seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW5D_51Q1r7A"
   },
   "source": [
    "Similar to the original paper we will use the Kaiming Initialization for weights with considering the reaky ReLU as activation function except the first layer and bias. We initialize the bias terms zero, and for the first layer we will use uniform distribution between [-1,1].\n",
    "\n",
    "After creating DPNet object we can easily try to train the model with training samples with {meth}.fit which takes training or validation dataloader as its single input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:16.314193Z",
     "start_time": "2024-03-18T16:04:16.310131Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710415203689,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "JmES6MD53vPx"
   },
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for p in model.parameters():\n",
    "        psh = p.shape\n",
    "        if len(psh) == 2:  # Linear layers\n",
    "            _, in_shape = psh\n",
    "            if in_shape == 2:  # Initial layer\n",
    "                torch.nn.init.uniform_(p, -1, 1)\n",
    "            else:\n",
    "                acname = model.activation.__name__.lower()\n",
    "                if acname == \"leakyrelu\":\n",
    "                    acname = \"leaky_relu\"\n",
    "                torch.nn.init.kaiming_uniform_(p, a=1, nonlinearity=acname)\n",
    "        else:  # Bias\n",
    "            torch.nn.init.zeros_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:24.300811Z",
     "start_time": "2024-03-18T16:04:16.744797Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105734,
     "status": "ok",
     "timestamp": 1710415309422,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "XLmZSrgm1eO9",
    "outputId": "8d77315d-b161-422e-e9f8-e2fd36febb19"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "kaiming_init(dpnet_fmap.lightning_module.encoder)\n",
    "kaiming_init(dpnet_fmap.lightning_module.lagged_encoder)\n",
    "dpnet_fmap.fit(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzRBZBFfpkcB"
   },
   "source": [
    "## Evaluation Metrics\n",
    "Now it is time to evaluate the performance of model. To do this, for this specific example we will use two different metrics that are used in the paper: (i) the optimality gap, and (ii) the spectral error.\n",
    "\n",
    "**Optimality Gap**\n",
    "\n",
    "The definition of the optimality gap in the paper is $\n",
    "\\sum_{i = 1}^{3} \\sigma_{i}^{2}(\\tau) - \\mathcal{P}^{0}(w)$, which informs on how close one is to capture the best rank-r approximation of the transfer operator, $\\mathcal{T}$.\n",
    "\n",
    "**Spectral Error**\n",
    "\n",
    "The spectral error is calculated by $ max_i \\ min_j \\left | \\lambda_i(\\mathcal{P}_{\\mathcal{H}}\\tau_{|\\mathcal{H}}) - \\lambda_j(\\tau) \\right |$. This formula measures how well the true eigenbalues of $\\mathcal{T}$ can be recovered within the representation space $\\mathcal{H_w}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PruYq_ppWck"
   },
   "source": [
    "In order to calculate the above metrics for evaluation the performance of the learned representation we should first calculate populational covariances and cross-covariances for the learned feature map.\n",
    "\n",
    "\n",
    "The population_covs function computes population covariance and cross-covariance matrices based on given feature and logistic maps. It begins by generating input data points and then calculates eigenvalues and left eigenfunctions of the logistic map. Subsequently, it normalizes the Perron eigenvector and evaluates the feature map on the input data to obtain a feature matrix. Covariance and cross-covariance matrices are then computed using the feature matrix and normalized Perron eigenvector.\n",
    "\n",
    "On the other hand, the evaluate_representation function assesses the performance of the obtained representation. It computes the OLS estimator, estimates eigenvalues, and evaluates metrics such as the optimality gap and spectral error to provide insights into the model's quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:26.755873Z",
     "start_time": "2024-03-18T16:04:26.749306Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415317409,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "XY8uZGgq7AWg"
   },
   "outputs": [],
   "source": [
    "from kooplearn.abc import FeatureMap\n",
    "from scipy.integrate import romb\n",
    "\n",
    "def population_covs(feature_map: FeatureMap, logistic: LogisticMap, pow_of_two_k: int = 12):\n",
    "    \"\"\"Computes the population covariance and cross-covariance\"\"\"\n",
    "    x = np.linspace(0, 1, 2**pow_of_two_k + 1)[:, None]\n",
    "    vals, lv = logistic.eig(eval_left_on=x) # returns eigenvalues and left eigenfunctions\n",
    "    perron_eig_idx = np.argmax(np.abs(vals))\n",
    "    pi = lv[:, perron_eig_idx] #eigenfunction correspond with the max eigenvalue\n",
    "    assert np.isreal(pi).all()\n",
    "    pi = pi.real\n",
    "    pi = pi / romb(pi, dx=1 / 2**pow_of_two_k)  # Normalization of π\n",
    "    # Evaluating the feature map\n",
    "    phi = feature_map(x)  # [2**pow_of_two_k + 1, d]\n",
    "    # Covariance\n",
    "    cov_unfolded = (\n",
    "        phi.reshape(2**pow_of_two_k + 1, -1, 1)\n",
    "        * phi.reshape(2**pow_of_two_k + 1, 1, -1)\n",
    "        * pi.reshape(-1, 1, 1)\n",
    "    )\n",
    "    cov = romb(cov_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "    # Cross-covariance\n",
    "    alphas = np.stack(\n",
    "        [logistic.noise_feature_composed_map(x, n) for n in range(logistic.N + 1)],\n",
    "        axis=1,\n",
    "    )\n",
    "    betas = np.stack(\n",
    "        [logistic.noise_feature(x, n) for n in range(logistic.N + 1)], axis=1\n",
    "    )\n",
    "\n",
    "    cov_alpha_unfolded = (\n",
    "        phi.reshape(2**pow_of_two_k + 1, -1, 1)\n",
    "        * alphas.reshape(2**pow_of_two_k + 1, 1, -1)\n",
    "        * pi.reshape(-1, 1, 1)\n",
    "    )\n",
    "    cov_beta_unfolded = phi.reshape(2**pow_of_two_k + 1, -1, 1) * betas.reshape(\n",
    "        2**pow_of_two_k + 1, 1, -1\n",
    "    )\n",
    "\n",
    "    cov_alpha = romb(cov_alpha_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "    cov_beta = romb(cov_beta_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "\n",
    "    cross_cov = cov_alpha @ (cov_beta.T)\n",
    "    return cov, cross_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:27.193581Z",
     "start_time": "2024-03-18T16:04:27.188757Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415320580,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3N2IkYQZ6ISP"
   },
   "outputs": [],
   "source": [
    "from kooplearn._src.metrics import directed_hausdorff_distance\n",
    "from kooplearn._src.utils import topk\n",
    "\n",
    "def evaluate_representation(feature_map: FeatureMap, logistic_map: LogisticMap):\n",
    "    report = {}\n",
    "    # Compute OLS estimator\n",
    "    cov, cross_cov = population_covs(feature_map, logistic_map)\n",
    "    OLS_estimator = np.linalg.solve(cov, cross_cov)\n",
    "\n",
    "    # Eigenvalue estimation\n",
    "    OLS_eigs = np.linalg.eigvals(OLS_estimator)\n",
    "    top_eigs = topk(np.abs(OLS_eigs), 3)\n",
    "    OLS_eigs = OLS_eigs[top_eigs.indices]\n",
    "\n",
    "    true_eigs = logistic_map.eig()\n",
    "    top_eigs = topk(np.abs(true_eigs), 3)\n",
    "    true_eigs = true_eigs[top_eigs.indices]\n",
    "\n",
    "    report[\"hausdorff-distance\"] = directed_hausdorff_distance(OLS_eigs, true_eigs)\n",
    "    # VAMP2-score\n",
    "    M = np.linalg.multi_dot(\n",
    "        [\n",
    "            np.linalg.pinv(cov, hermitian=True),\n",
    "            cross_cov,\n",
    "            np.linalg.pinv(cov, hermitian=True),\n",
    "            cross_cov.T,\n",
    "        ]\n",
    "    )\n",
    "    feature_dim = cov.shape[0]\n",
    "    report[\"optimality-gap\"] = np.sum(logistic_map.svals()[:feature_dim] ** 2) - np.trace(M)\n",
    "\n",
    "    # Feasibility\n",
    "    # report[\"feasibility-gap\"] = np.linalg.norm(cov - np.eye(feature_dim), ord=2)\n",
    "    # report[\"estimator-eigenvalues\"] = OLS_eigs\n",
    "    # report[\"covariance-eigenvalues\"] = np.linalg.eigvalsh(cov)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzGMOWhVr0SR"
   },
   "source": [
    "Now we can assess the performace of the learned representation based on optimality gap and spectral error or hausdorff distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:30.048810Z",
     "start_time": "2024-03-18T16:04:29.944559Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1710415330683,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "91WBcVsRrztw",
    "outputId": "6c7f0b9e-3219-4709-f455-b5b9bbf256c7"
   },
   "outputs": [],
   "source": [
    "evaluate_representation(dpnet_fmap, logmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QegCVjdyrzbX"
   },
   "source": [
    "As we saw the result is aligned with the reported results in the paper, however we only run for one seed. Therefore, to establish a more meaningful statistical comparison between different models which tries to learn the representation we run each models for 20 independent runs and report mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slYYDYwmqXvR"
   },
   "source": [
    "## Evaluating DPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt0QAfHzu0Dk"
   },
   "source": [
    "Now we run the DPNet over 20 independent runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:07:36.606017Z",
     "start_time": "2024-03-18T16:05:03.458155Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1286092,
     "status": "ok",
     "timestamp": 1710417204771,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "drxXOF6gYvUV",
    "outputId": "e29f801a-45f6-4550-d026-67bb958c011d"
   },
   "outputs": [],
   "source": [
    "relaxed = False\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    dpnet_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        DPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = DPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(dpnet_fmap.lightning_module.encoder)\n",
    "    kaiming_init(dpnet_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    dpnet_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(dpnet_fmap, logmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRd8x0k9uihM"
   },
   "source": [
    "Now we calculate mean and standard deviation for each metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:22.823847Z",
     "start_time": "2024-03-18T16:11:22.819913Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710417242774,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "pVoxJYT76UVF"
   },
   "outputs": [],
   "source": [
    "def stack_reports(reports):\n",
    "    stacked = {}\n",
    "    for key in [\n",
    "        \"hausdorff-distance\",\n",
    "        \"optimality-gap\",\n",
    "        # \"feasibility-gap\",\n",
    "        # \"fit-time\",\n",
    "        # \"time_per_epoch\",\n",
    "    ]:\n",
    "        if key in reports[0]:\n",
    "            stacked[key] = np.mean([r[key] for r in reports])\n",
    "            key_std = key + \"_std\"\n",
    "            stacked[key_std] = np.std([r[key] for r in reports])\n",
    "    # for key in [\"estimator-eigenvalues\", \"covariance-eigenvalues\"]:\n",
    "    #     stacked[key] = np.stack([r[key] for r in reports])\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:23.881292Z",
     "start_time": "2024-03-18T16:11:23.878470Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710417242774,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "NA7coxb1uhsK"
   },
   "outputs": [],
   "source": [
    "full_DPNet_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:25.231580Z",
     "start_time": "2024-03-18T16:11:25.227901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1710417264732,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "x6H5D8NdAex1",
    "outputId": "f868d009-cf17-4664-e620-0ab448bcc323"
   },
   "outputs": [],
   "source": [
    "full_DPNet_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA3C9Aar2pZM"
   },
   "source": [
    "## Evaluating DPNet relaxed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqlCahty2pZS"
   },
   "source": [
    "Now we run the DPNet-relaxed over 20 independent runs. To do so we just need to change the flag that we pass to the DPNet module to whether use the relaxed version or not. For more information about the difference between the relaxed version of loss function with the original read the section 3 of the paper \\cite{}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:15:28.102586Z",
     "start_time": "2024-03-18T16:12:56.603527Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1967449,
     "status": "ok",
     "timestamp": 1710420130814,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "WEGvBWHz2pZS",
    "outputId": "8fcce119-3076-4199-db67-f687c5384ef1"
   },
   "outputs": [],
   "source": [
    "DPNets = {'loss_fn': DPLoss, 'loss_kwargs': {'relaxed': True, 'metric_deformation': metric_deformation_coeff, 'center_covariances': False}}\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    dpnet_relaxed_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        DPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = DPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(dpnet_relaxed_fmap.lightning_module.encoder)\n",
    "    kaiming_init(dpnet_relaxed_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    dpnet_relaxed_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(dpnet_relaxed_fmap, logmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:22:21.767884Z",
     "start_time": "2024-03-18T16:22:21.764713Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710420130815,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "nRh1Dnzs2pZS"
   },
   "outputs": [],
   "source": [
    "full_DPNet_relaxed_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:22:22.653803Z",
     "start_time": "2024-03-18T16:22:22.650129Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710420130815,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "IRwc_QW82pZS",
    "outputId": "05a077b8-b6dc-4c3e-b5f5-93ae5fbf9ace"
   },
   "outputs": [],
   "source": [
    "full_DPNet_relaxed_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKzufK-C3j_x"
   },
   "source": [
    "## Evaluating VAMPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Frg-JO6h3j_x"
   },
   "source": [
    "Now we run the VAMPNet \\cite{} over 20 independent runs. To do so we just need to import the VAMPNet class from the kooplearn's feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:26:16.341495Z",
     "start_time": "2024-03-18T16:26:16.338806Z"
    },
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1710420206199,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "ID4wDzbz4-Mo"
   },
   "outputs": [],
   "source": [
    "from kooplearn.nn import VAMPLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:28:44.980486Z",
     "start_time": "2024-03-18T16:26:16.770848Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4v-mbRn3j_x"
   },
   "outputs": [],
   "source": [
    "VAMPNets = {'loss_fn': VAMPLoss, 'loss_kwargs': {'schatten_norm': 2, 'center_covariances': False}}\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    vampnet_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        VAMPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = VAMPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(vampnet_fmap.lightning_module.encoder)\n",
    "    kaiming_init(vampnet_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    vampnet_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(vampnet_fmap, logmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:36.613994Z",
     "start_time": "2024-03-18T16:31:36.610924Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "WjI9vyct3j_y"
   },
   "outputs": [],
   "source": [
    "full_VAMPNet_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:37.383407Z",
     "start_time": "2024-03-18T16:31:37.379939Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "5veUFzO13j_y"
   },
   "outputs": [],
   "source": [
    "full_VAMPNet_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBI5eneg7tEJ"
   },
   "source": [
    "## Evaluating Noise Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:44.539383Z",
     "start_time": "2024-03-18T16:31:44.536826Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "kSmZolYS80CF"
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "from kooplearn.models.feature_maps import ConcatenateFeatureMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:54.956597Z",
     "start_time": "2024-03-18T16:31:54.953001Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "9tSyPOpz8DhJ"
   },
   "outputs": [],
   "source": [
    "def noise_feat(n, x):\n",
    "    return logmap.noise_feature(x, n)\n",
    "\n",
    "def NoiseKernel(order: int = 3):\n",
    "    binom_coeffs = [scipy.special.binom(N_exp, i) for i in range(N_exp + 1)]\n",
    "    sorted_coeffs = np.argsort(binom_coeffs)\n",
    "\n",
    "    fn_list = [partial(noise_feat, n) for n in sorted_coeffs[:order]]\n",
    "\n",
    "    return ConcatenateFeatureMaps(fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:46.565758Z",
     "start_time": "2024-03-18T16:32:46.501398Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "68y28MYY-xA4"
   },
   "outputs": [],
   "source": [
    "full_NoiseKernel_report = evaluate_representation(NoiseKernel(feature_dim), logistic_map=logmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:47.849959Z",
     "start_time": "2024-03-18T16:32:47.846465Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "2bWL8K-8-sNl"
   },
   "outputs": [],
   "source": [
    "full_NoiseKernel_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4j-uK_W_mW7"
   },
   "source": [
    "## Evaluating Cheby-T Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:54.930220Z",
     "start_time": "2024-03-18T16:32:54.926897Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "-70rxpVE_rVa"
   },
   "outputs": [],
   "source": [
    "def scaled_chebyt(n, x):\n",
    "    return scipy.special.eval_chebyt(n, 2 * x - 1)\n",
    "\n",
    "def ChebyT(feature_dim: int = 3):\n",
    "\n",
    "    fn_list = [partial(scaled_chebyt, n) for n in range(feature_dim)]\n",
    "    return ConcatenateFeatureMaps(fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:59.542982Z",
     "start_time": "2024-03-18T16:32:59.483130Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "UxadJZCu_zSD"
   },
   "outputs": [],
   "source": [
    "full_ChebyTKenel_report = evaluate_representation(ChebyT(feature_dim),logmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:33:00.087502Z",
     "start_time": "2024-03-18T16:33:00.083998Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "_MUnVAPU_4_3"
   },
   "outputs": [],
   "source": [
    "full_ChebyTKenel_report"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
