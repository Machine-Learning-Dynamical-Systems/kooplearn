{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The metastable states of a molecule\n",
    "In this example we will reproduce the results in {footcite:t}`Mardt2018`, training a {class}`kooplearn.models.feature_maps.VAMPNet` to learn the kinetics of the small molecule Alanine Dipeptide from simulation data. \n",
    "\n",
    "## Description of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "data_path = Path.cwd().parent.parent / \"examples/ala2/__data__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/pietronovelli/code_repos/kooplearn/examples/ala2/__data__')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ala2_data(model_path: os.PathLike, descriptor:str):\n",
    "    rel_path = model_path\n",
    "    files = {\n",
    "        \"dihedrals\": \"alanine-dipeptide-3x250ns-backbone-dihedrals.npz\",\n",
    "        \"distances\": \"alanine-dipeptide-3x250ns-heavy-atom-distances.npz\",\n",
    "        \"positions\": \"alanine-dipeptide-3x250ns-heavy-atom-positions.npz\",\n",
    "    }\n",
    "    if descriptor not in ['dihedrals', 'distances', 'positions']:\n",
    "        raise ValueError(f\"descriptor must be one of 'dihedrals', 'distances', 'positions'. Got {descriptor}\")\n",
    "    return np.concatenate([np.load(os.path.join(rel_path, files[descriptor]))[f\"arr_{arr_idx}\"] for arr_idx in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = load_ala2_data(data_path, 'distances')\n",
    "dihedrals = load_ala2_data(data_path, 'dihedrals')\n",
    "\n",
    "distances_dim = distances.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The provided trajectory is of type <class 'numpy.ndarray'>. Converting to torch.Tensor.\n"
     ]
    }
   ],
   "source": [
    "# Make the data into a context window Dataset\n",
    "from kooplearn.nn.data import traj_to_contexts_dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dist_dataset = traj_to_contexts_dataset(distances)\n",
    "train_dist, val_dist, test_dist = random_split(dist_dataset, [0.8, 0.1, 0.1])\n",
    "train_loader = DataLoader(train_dist, batch_size=10000, shuffle=True)\n",
    "val_loader = DataLoader(val_dist, batch_size=5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class _old_MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim: int, activation=torch.nn.ELU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feature_dim,64), torch.nn.LayerNorm(64), self.activation(),\n",
    "            torch.nn.Linear(64,128), self.activation(),\n",
    "            torch.nn.Linear(128,64), self.activation(),\n",
    "            torch.nn.Linear(64,8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim: int, activation=torch.nn.ELU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(feature_dim),\n",
    "            torch.nn.Linear(feature_dim, 20), torch.nn.ELU(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.ELU(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.ELU(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.ELU(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.ELU(),\n",
    "            torch.nn.Linear(20, 6), torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from kooplearn.models.feature_maps import VAMPNet, DPNet\n",
    "from torch.optim import Adam\n",
    "import lightning \n",
    "\n",
    "\n",
    "trainer_kwargs = {\n",
    "    \"accelerator\": \"cpu\",\n",
    "    \"devices\": 1,\n",
    "    \"max_epochs\": 30,\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"enable_model_summary\": True,\n",
    "    \"enable_checkpointing\": False,\n",
    "    \"logger\": False,\n",
    "}\n",
    "\n",
    "\n",
    "trainer = lightning.Trainer(**trainer_kwargs)\n",
    "feature_map = DPNet(MLP, Adam, trainer, encoder_kwargs={'feature_dim': distances_dim}, optimizer_kwargs={\"lr\": 1e-3},center_covariances=False,\n",
    "        seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name    | Type | Params\n",
      "---------------------------------\n",
      "0 | encoder | MLP  | 2.8 K \n",
      "---------------------------------\n",
      "2.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting DPNet. Lookback window length set to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f2bf8d1643456193bf5d8a3032d3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/core/module.py:478: UserWarning: You called `self.log('train/projection_score', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "  rank_zero_warn(\n",
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/core/module.py:478: UserWarning: You called `self.log('train/metric_deformation_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "  rank_zero_warn(\n",
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/core/module.py:478: UserWarning: You called `self.log('train/total_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "  rank_zero_warn(\n",
      "/Users/pietronovelli/anaconda3/envs/kooplearn/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "feature_map.fit(train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kooplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
