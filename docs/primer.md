(primer)=
# A friendly primer on Evolution Operator learning

Evolution operator learning is a data-driven approach to characterizing dynamical systems, which can be either stochastic, $x_{t + 1} \sim p(\cdot | x_{t})$, or deterministic, $x_{t+1} \sim \delta(\cdot - F(x_{t}))$. Throughout, we assume the dynamics to be **Markovian**, so that the evolution of $x_t$ depends on $x_{t}$ alone and not on the states at times $s < t$. If this assumption is not satisfied by $x_{t}$, a standard trick is to re-define the state as a context $c_{t}^{H} = f(x_{t}, x_{t -1}, \ldots, x_{t - H})$ with history length $H$, where $f$ can be a simple concatenation (as implemented by {class}`kooplearn.preprocessing.TimeDelayEmbedding`) or a learned sequence model (e.g., a recurrent neural network or transformer).

***Evolution operators*** are defined as follows: for every function $f$ of the state of the system, $(\mathsf{E} f)(x_{t})$ is the expected value of $f$ one step ahead in the future, given that at time $t$ the system was found in $x_t$:

$$
(\mathsf{E} f)(x_t) = \int p(dy | x_{t}) f(y) = \mathbb{E}_{y \sim X_{t + 1} | X_{t}}[f(y) | x_t].
$$

Notice that $\mathsf{E}$ is an operator because it maps any function $f$ to another function, $x_{t} \mapsto (\mathsf{E} f)(x_t)$, and is ***linear*** because $\mathsf{E}(f + \alpha g) = \mathsf{E} f + \alpha \mathsf{E} g$. When the dynamics is deterministic, $\mathsf{E}$ is known as the ***Koopman operator*** {cite:p}`Koopman1931`, while in the stochastic case it is known as the ***transfer operator*** {cite:p}`Applebaum2009`.

Evolution operators fully characterize the dynamical system because knowing $\mathsf{E}$ allows us to reconstruct the dynamical law $p(\cdot | x_{t})$. Indeed, for any subset of the state space $B \subseteq \mathcal{X}$, applying $\mathsf{E}$ to the indicator function of $B$, we have

$$
(\mathsf{E} 1_{B})(x_t) = \int_{B}p(dy|x_{t}) = \mathbb{P}\left[X_{t + 1} \in B | x_t\right].
$$

An advantage of the operator approach over dealing directly with the conditional probability $p(\cdot | x_{t})$ is that $\mathsf{E}$ acts linearly on the objects to which it is applied. This means that operators unlock an arsenal of tools from linear algebra and functional analysis, which would be unavailable otherwise. Arguably the most important of them is the **spectral decomposition**, allowing us to decompose $\mathsf{E}$, and hence the dynamics, into a linear superposition of dynamical modes. These ideas lie at the core of the celebrated Time-lagged Independent Component Analysis (TICA) {cite:p}`Molgedey1994,perez2013identification` and Dynamical Mode Decomposition (DMD) {cite:p}`Schmid2010, Kutz2016`.

## Learning $\mathsf{E}$ and its spectral decomposition from data

```{figure} _static/operator_learning.svg
:align: right

A schematic of the operator learning framework.
```

We now review the main approaches to learn the evolution operator and its spectral decomposition from a finite dataset of observations.

A core idea of operator learning is that operators are defined by how they act *on a suitable linear space of functions*, similarly to how matrices are defined by their action on a basis of vectors. Of course, not every function $f$ is interesting, and this nicely parallels with the matrix example, where the most "interesting" directions are those that recover most of the variance in the data.

Learning $\mathsf{E}$, therefore, is usually cast as the following problem:

```{note}
**The Learning Problem:**
Letting $\varphi(x) \in \mathbb{R}^{d}$ be a --- learned or fixed --- encoder of the state, find the best approximation of $\mathsf{E}$ *restricted* to the $d$-dimensional linear space of functions generated by $\varphi$, given the data.
```

In practice, the data is usually a collection of transitions $\mathcal{D} = (x_i, y_i)_{i = 1}^{N}$, where it is intended that $x_{i} \sim \mathbb{P}[X_{t}]$ are sampled from a distribution of initial states, while $y_{i} \sim p( \cdot | x_{i})$.

### Least squares

In this approach, the encoder $\varphi$ is a frozen, non-learnable dictionary of functions, and we are interested in approximating the action of $\mathsf{E}$ on functions of the form $f(x) = \langle w,\varphi(x)\rangle$ for every $w \in \mathbb{R} ^{d}$. To this end, one minimizes the empirical error between the true conditional expectation $\mathbb{E}_{y \sim X_{t + 1} | X_{t}}[\langle w,\varphi(y)\rangle | x]$, and a linear model $ \langle Ew,\varphi(x)\rangle$, where the matrix $E \in \mathbb{R} ^{d \times d}$ identifies the restriction of the evolution operator to the linear span of the dictionary:

$$
\frac{1}{N}\sum_{i = 1}^{N} (\langle w,\varphi(y_{i})\rangle - \langle Ew,\varphi(x_i)\rangle)^2 \leq \frac{1}{N}\sum_{i = 1}^{N} \|\varphi(y_i) - E^\top \varphi(x_i)\|^{2} + \lambda \|E\|^{2}.
$$

On the right-hand side, we assumed $\|w\| \leq 1$, used the Cauchy–Schwarz inequality, and added a ridge penalty. The minimizer of the equation above can be computed in closed form {cite:p}`korda2018convergence, Kostic2022` as:

$$
\mathsf{E}_{ls} = (\text{cov}_{X} + \lambda\mathsf{I})^{-1}\text{cov}_{XY}, \quad \text{with }\text{cov}_{XY} = \frac{1}{N} \sum_{i = 1}^N\varphi(x_i)\varphi(y_i)^\top \text{and } \text{cov}_{X} = \text{cov}_{XX}.
$$

In the limit of infinite data, $N\to \infty$, and infinitely dimensional encoders, $d \to \infty$, the least squares estimator converges {cite:p}`korda2018convergence` in the strong operator topology to the evolution operator $\mathsf{E}$, and similar (but weaker) asymptotic convergence results are proved for its spectrum. When $\varphi$ is fixed and finite-dimensional, the least squares estimator for evolution operator learning is implemented in {class}`kooplearn.linear_model.Ridge`.

### Dynamical Modes

The spectral decomposition of $\mathsf{E}$ is approximated by expressing the least-squares estimator in its eigenvectors' basis $\mathsf{E}_{ls} = Q\Lambda Q^{-1}$, where the columns of $Q = [q_1, \cdots, q_d]$ are the eigenvectors of $\mathsf{E}_{ls}$, and $\Lambda$ is a diagonal matrix of eigenvalues. In this basis, the expected value in the future for a function $f(x) = \langle w,\varphi(x)\rangle$ is expressed as:

$$
\mathbb{E}_{y \sim X_{t + 1} | X_{t}}[f(y) | x] \approx \langle \mathsf{E}_{ls} w,\varphi(x)\rangle = \langle Q\Lambda Q^{-1}w,\varphi(x)\rangle
= \sum_{i = 1}^{d} \lambda_i \langle q_i,\varphi(x)\rangle (Q^{-1}w)_{i}.
$$
The spectral decomposition expresses the transition $x_t \to x_{t+1}$ as a sum of ***modes*** of the form $\lambda_i \langle q_i,\varphi(x)\rangle (Q^{-1}w)_i$, each of which can be broken down into three components:

1.  The **eigenvalues** $\lambda_i$ determine the time scales of the transition. Indeed, applying the evolution operator $s$ times to analyze the transition $x_t \to x_{t+s}$ leaves the eigenvalue decomposition unchanged, except that each $\lambda_i$ becomes $\lambda_i^s$. Writing $\lambda_i^s = \rho_i^s e^{i s \omega_i}$ in polar coordinates, reveals that the modes decay exponentially over time with rate $\rho_i$, while oscillating at frequency $\omega_i$.

2.  The initial state $x$ influences the decomposition through the factor $\Psi_{i}(x) = \langle q_i,\varphi(x)\rangle$. This coefficient captures how strongly the state $x$ aligns with the $i$-th mode. When $q_i$ corresponds to an eigenvalue with slow decay, i.e., $|\lambda_i| \approx 1$, the term $\Psi_{i}(x)$ serves as a natural quantity for clustering states into ***coherent*** or ***metastable*** sets.

3.  The coefficient $(Q^{-1}w)_i$, in turn, indicates how the function represented by the vector $w$ relates to the $i$-th mode. This connection makes it possible to link the dynamical patterns to specific functions — or ***observables*** — thereby deepening our understanding of the system.

### Kernel methods

Leveraging the kernel trick, one can learn evolution operators by deriving a closed-form solution of the least square loss in terms of kernel matrices whose elements are of the form $k(x_i, x_j) = \langle \varphi(x_i),\varphi(x_j)\rangle$, with $k(\cdot, \cdot)$ a suitable kernel function. Thanks to the theory of reproducing kernel Hilbert spaces, this class of methods is backed up by statistical learning guarantees, such as the ones derived in {cite:p}`Kostic2022,Kostic2023SpectralRates,nuske2023finite`. Similarly to the least-squares approach, one also approximates the spectral decomposition of $\mathsf{E}$ via kernel methods {cite:p}`Williams2015_KDMD,Kawahara2016, Klus2019, Das2020, Alexander2020, Meanti2023`. See the module {mod}`kooplearn.kernel` for state-of-the-art kernel methods on evolution operator learning.

### Deep learning

In contrast to the previous approaches, where the encoder $\varphi$ is prescribed, a number of methods proposed to approximate $\mathsf{E}$ from data with end-to-end schemes including $\varphi$ as a learnable neural network. Since learning $\mathsf{E}$ ultimately entails learning its action on the linear space spanned by $\varphi$, it is appealing to choose an encoder capturing the most salient features of the dynamics.

To this end, one can train $\varphi$ via an ***encoder-decoder*** scheme as proposed in {cite:p}`takeishi2017learning,Lusch2018, otto2019linearly, Azencot2020CAE, wehmeyer2018time` or with ***encoder-only*** approaches as in {cite:p}`li2017extended,Mardt2018,yeung2019learning,Kostic2023DPNets,federici2023latent, turri2025self`.

---

```{bibliography}
:filter: docname in docnames
:style: unsrt
```