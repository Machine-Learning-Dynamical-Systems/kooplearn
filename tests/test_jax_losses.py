import jax
import jax.numpy as jnp
import pytest

from kooplearn.jax.nn._functional import (
    autoencoder_loss,
    energy_loss,
    orthonormal_fro_reg,
    orthonormal_logfro_reg,
    spectral_contrastive_loss,
    vamp_loss,
)


@pytest.fixture
def sample_features():
    """Create sample feature arrays."""
    key = jax.random.PRNGKey(42)
    batch_size = 16
    feature_dim = 10
    x = jax.random.normal(key, (batch_size, feature_dim))
    y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, feature_dim))
    return x, y


@pytest.fixture
def large_batch_features():
    """Create larger sample feature arrays."""
    key = jax.random.PRNGKey(42)
    batch_size = 128
    feature_dim = 50
    x = jax.random.normal(key, (batch_size, feature_dim))
    y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, feature_dim))
    return x, y


@pytest.fixture
def single_sample_features():
    """Create single sample feature arrays."""
    key = jax.random.PRNGKey(42)
    x = jax.random.normal(key, (1, 10))
    y = jax.random.normal(jax.random.fold_in(key, 1), (1, 10))
    return x, y


class TestVampLoss:
    """Test suite for vamp_loss functional."""

    def test_default_params(self, sample_features):
        """Test vamp_loss with default parameters."""
        x, y = sample_features
        loss = vamp_loss(x, y)

        assert isinstance(loss, jnp.ndarray)
        assert loss.shape == ()  # Scalar
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_schatten_norm_1(self, sample_features):
        """Test vamp_loss with Schatten norm p=1."""
        x, y = sample_features
        loss = vamp_loss(x, y, schatten_norm=1)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_schatten_norm_2(self, sample_features):
        """Test vamp_loss with Schatten norm p=2."""
        x, y = sample_features
        loss = vamp_loss(x, y, schatten_norm=2)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_schatten_norm_invalid(self, sample_features):
        """Test vamp_loss with invalid Schatten norm."""
        x, y = sample_features

        with pytest.raises(NotImplementedError):
            vamp_loss(x, y, schatten_norm=3)

    def test_centered_covariances(self, sample_features):
        """Test vamp_loss with centered covariances."""
        x, y = sample_features
        loss = vamp_loss(x, y, center_covariances=True)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_uncentered_covariances(self, sample_features):
        """Test vamp_loss with uncentered covariances."""
        x, y = sample_features
        loss = vamp_loss(x, y, center_covariances=False)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_symmetric_inputs(self, sample_features):
        """Test vamp_loss with identical inputs."""
        x, _ = sample_features
        loss = vamp_loss(x, x)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_different_batch_sizes(self):
        """Test vamp_loss with different batch sizes.

        Note: JAX returns NaN with centered covariances for batch_size=1,
        so we use uncentered covariances here. PyTorch handles this gracefully.
        """
        key = jax.random.PRNGKey(42)

        for batch_size in [1, 2, 8, 32]:
            x = jax.random.normal(key, (batch_size, 10))
            y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 10))
            loss = vamp_loss(x, y, center_covariances=False)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_single_sample(self, single_sample_features):
        """Test vamp_loss with single sample - JAX returns NaN with centered covariances."""
        x, y = single_sample_features

        # JAX returns NaN with centered covariances for single sample
        loss_centered = vamp_loss(x, y, center_covariances=True)
        assert jnp.isnan(loss_centered)

        # But works fine with uncentered covariances
        loss_uncentered = vamp_loss(x, y, center_covariances=False)
        assert loss_uncentered.shape == ()
        assert not jnp.isnan(loss_uncentered)

    def test_large_batch(self, large_batch_features):
        """Test vamp_loss with large batch size."""
        x, y = large_batch_features
        loss = vamp_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_dtype_preservation(self, sample_features):
        """Test that vamp_loss preserves dtype."""
        x, y = sample_features
        loss = vamp_loss(x, y)

        assert loss.dtype == x.dtype

    def test_jit_compilation(self, sample_features):
        """Test that vamp_loss can be JIT-compiled."""
        x, y = sample_features
        jitted_loss = jax.jit(lambda x, y: vamp_loss(x, y))
        loss = jitted_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_gradient_computation(self, sample_features):
        """Test gradient computation through vamp_loss."""
        x, y = sample_features

        def loss_fn(x, y):
            return vamp_loss(x, y)

        grad_x = jax.grad(loss_fn, argnums=0)(x, y)
        grad_y = jax.grad(loss_fn, argnums=1)(x, y)

        assert grad_x.shape == x.shape
        assert grad_y.shape == y.shape
        assert not jnp.isnan(grad_x).any()
        assert not jnp.isnan(grad_y).any()


class TestSpectralContrastiveLoss:
    """Test suite for spectral_contrastive_loss functional."""

    def test_basic_functionality(self, sample_features):
        """Test basic spectral_contrastive_loss computation."""
        x, y = sample_features
        loss = spectral_contrastive_loss(x, y)

        assert isinstance(loss, jnp.ndarray)
        assert loss.shape == ()  # Scalar
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_symmetric_inputs(self, sample_features):
        """Test spectral_contrastive_loss with identical inputs."""
        x, _ = sample_features
        loss = spectral_contrastive_loss(x, x)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_shape_mismatch_raises(self):
        """Test that shape mismatch raises assertion."""
        key = jax.random.PRNGKey(42)
        x = jax.random.normal(key, (16, 10))
        y = jax.random.normal(jax.random.fold_in(key, 1), (16, 5))

        with pytest.raises(ValueError):
            spectral_contrastive_loss(x, y)

    def test_different_batch_sizes(self):
        """Test spectral_contrastive_loss with different batch sizes."""
        key = jax.random.PRNGKey(42)

        for batch_size in [2, 4, 8, 32]:
            x = jax.random.normal(key, (batch_size, 10))
            y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 10))
            loss = spectral_contrastive_loss(x, y)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_large_batch(self, large_batch_features):
        """Test spectral_contrastive_loss with large batch size."""
        x, y = large_batch_features
        loss = spectral_contrastive_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_different_feature_dims(self):
        """Test spectral_contrastive_loss with different feature dimensions."""
        key = jax.random.PRNGKey(42)

        for feat_dim in [5, 10, 50, 100]:
            x = jax.random.normal(key, (16, feat_dim))
            y = jax.random.normal(jax.random.fold_in(key, 1), (16, feat_dim))
            loss = spectral_contrastive_loss(x, y)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_dtype_preservation(self, sample_features):
        """Test that spectral_contrastive_loss preserves dtype."""
        x, y = sample_features
        loss = spectral_contrastive_loss(x, y)

        assert loss.dtype == x.dtype

    def test_jit_compilation(self, sample_features):
        """Test that spectral_contrastive_loss can be JIT-compiled."""
        x, y = sample_features
        jitted_loss = jax.jit(spectral_contrastive_loss)
        loss = jitted_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_gradient_computation(self, sample_features):
        """Test gradient computation through spectral_contrastive_loss."""
        x, y = sample_features

        grad_x = jax.grad(spectral_contrastive_loss, argnums=0)(x, y)
        grad_y = jax.grad(spectral_contrastive_loss, argnums=1)(x, y)

        assert grad_x.shape == x.shape
        assert grad_y.shape == y.shape
        assert not jnp.isnan(grad_x).any()
        assert not jnp.isnan(grad_y).any()

    def test_orthogonal_features(self):
        """Test spectral_contrastive_loss with orthogonal features."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        feat_dim = 10

        # Create orthogonal features
        q, _ = jnp.linalg.qr(jax.random.normal(key, (feat_dim, feat_dim)))
        x = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 1)) @ q[:1, :]
        y = jax.random.normal(jax.random.fold_in(key, 2), (batch_size, 1)) @ q[1:2, :]

        loss = spectral_contrastive_loss(x, y)

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)


class TestAutoEncoderLoss:
    """Test suite for autoencoder_loss functional."""

    def test_default_params(self, sample_features):
        """Test autoencoder_loss with default parameters."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x + 0.01 * jax.random.normal(jax.random.PRNGKey(0), x.shape)
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y + 0.01 * jax.random.normal(jax.random.PRNGKey(3), y.shape)

        loss = autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        assert isinstance(loss, jnp.ndarray)
        assert loss.shape == ()  # Scalar
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_reconstruction_only(self, sample_features):
        """Test autoencoder_loss with reconstruction weight only."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x + 0.1 * jax.random.normal(jax.random.PRNGKey(0), x.shape)
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y

        loss = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=1.0,
            alpha_lin=0.0,
            alpha_pred=0.0,
        )

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_linearity_only(self, sample_features):
        """Test autoencoder_loss with linearity weight only."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y

        loss = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=0.0,
            alpha_lin=1.0,
            alpha_pred=0.0,
        )

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_prediction_only(self, sample_features):
        """Test autoencoder_loss with prediction weight only."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y + 0.1 * jax.random.normal(jax.random.PRNGKey(3), y.shape)

        loss = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=0.0,
            alpha_lin=0.0,
            alpha_pred=1.0,
        )

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_zero_weights(self, sample_features):
        """Test autoencoder_loss with all zero weights."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y

        loss = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=0.0,
            alpha_lin=0.0,
            alpha_pred=0.0,
        )

        assert jnp.allclose(loss, 0.0)

    def test_large_weights(self, sample_features):
        """Test autoencoder_loss with large weights."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x + 0.1 * jax.random.normal(jax.random.PRNGKey(0), x.shape)
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, latent_dim))
        y_pred = y + 0.1 * jax.random.normal(jax.random.PRNGKey(3), y.shape)

        loss = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=100.0,
            alpha_lin=100.0,
            alpha_pred=100.0,
        )

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_perfect_reconstruction(self, sample_features):
        """Test autoencoder_loss with perfect reconstructions."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, latent_dim))
        x_evo = y_enc  # Make evolved latent match encoded output
        y_pred = y

        loss = autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        # Should be zero since all three terms are perfectly satisfied
        assert jnp.isclose(loss, 0.0, atol=1e-6)

    def test_different_batch_sizes(self):
        """Test autoencoder_loss with different batch sizes."""
        key = jax.random.PRNGKey(42)

        for batch_size in [1, 2, 8, 32]:
            x = jax.random.normal(key, (batch_size, 10))
            y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 10))
            x_rec = x
            y_enc = jax.random.normal(jax.random.fold_in(key, 2), (batch_size, 5))
            x_evo = jax.random.normal(jax.random.fold_in(key, 3), (batch_size, 5))
            y_pred = y

            loss = autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_large_batch(self, large_batch_features):
        """Test autoencoder_loss with large batch size."""
        x, y = large_batch_features
        batch_size, _feat_dim = x.shape
        latent_dim = 10

        key = jax.random.PRNGKey(0)
        x_rec = x + 0.01 * jax.random.normal(key, x.shape)
        y_enc = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.fold_in(key, 2), (batch_size, latent_dim))
        y_pred = y + 0.01 * jax.random.normal(jax.random.fold_in(key, 3), y.shape)

        loss = autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_weighted_combination(self, sample_features):
        """Test that weights correctly scale loss components."""
        x, y = sample_features
        batch_size, _feat_dim = x.shape
        latent_dim = 5

        key = jax.random.PRNGKey(0)
        x_rec = x + jax.random.normal(key, x.shape)
        y_enc = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, latent_dim))
        x_evo = jax.random.normal(jax.random.fold_in(key, 2), (batch_size, latent_dim))
        y_pred = y + jax.random.normal(jax.random.fold_in(key, 3), y.shape)

        # Compute with different weight configurations
        loss1 = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=1.0,
            alpha_lin=0.0,
            alpha_pred=0.0,
        )
        loss2 = autoencoder_loss(
            x,
            y,
            x_rec,
            y_enc,
            x_evo,
            y_pred,
            alpha_rec=2.0,
            alpha_lin=0.0,
            alpha_pred=0.0,
        )

        # Loss should roughly double when weight doubles
        assert jnp.isclose(loss2, 2 * loss1, rtol=0.01)

    def test_dtype_preservation(self, sample_features):
        """Test that autoencoder_loss preserves dtype."""
        x, y = sample_features
        batch_size, _ = x.shape

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, 5))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, 5))
        y_pred = y

        loss = autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        assert loss.dtype == x.dtype

    def test_jit_compilation(self, sample_features):
        """Test that autoencoder_loss can be JIT-compiled."""
        x, y = sample_features
        batch_size, _ = x.shape

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, 5))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, 5))
        y_pred = y

        jitted_loss = jax.jit(autoencoder_loss)
        loss = jitted_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_gradient_computation(self, sample_features):
        """Test gradient computation through autoencoder_loss."""
        x, y = sample_features
        batch_size, _ = x.shape

        x_rec = x
        y_enc = jax.random.normal(jax.random.PRNGKey(1), (batch_size, 5))
        x_evo = jax.random.normal(jax.random.PRNGKey(2), (batch_size, 5))
        y_pred = y

        def loss_fn(x_rec, y_enc, x_evo, y_pred):
            return autoencoder_loss(x, y, x_rec, y_enc, x_evo, y_pred)

        grad_x_rec = jax.grad(loss_fn, argnums=0)(x_rec, y_enc, x_evo, y_pred)
        grad_y_enc = jax.grad(loss_fn, argnums=1)(x_rec, y_enc, x_evo, y_pred)
        grad_x_evo = jax.grad(loss_fn, argnums=2)(x_rec, y_enc, x_evo, y_pred)
        grad_y_pred = jax.grad(loss_fn, argnums=3)(x_rec, y_enc, x_evo, y_pred)

        assert grad_x_rec.shape == x_rec.shape
        assert grad_y_enc.shape == y_enc.shape
        assert grad_x_evo.shape == x_evo.shape
        assert grad_y_pred.shape == y_pred.shape


class TestOrthonormalFroReg:
    """Test suite for orthonormal_fro_reg functional."""

    def test_basic_functionality(self, sample_features):
        """Test basic orthonormal_fro_reg computation."""
        x, _ = sample_features
        key = jax.random.PRNGKey(0)
        reg = orthonormal_fro_reg(x, key)

        assert isinstance(reg, jnp.ndarray)
        assert reg.shape == ()  # Scalar
        assert not jnp.isnan(reg)
        assert not jnp.isinf(reg)

    def test_identity_covariance(self):
        """Test orthonormal_fro_reg with identity covariance (orthonormal features)."""
        key = jax.random.PRNGKey(0)
        # Create orthonormal features
        q, _ = jnp.linalg.qr(jax.random.normal(key, (10, 10)))
        x = q  # Already orthonormal
        reg = orthonormal_fro_reg(x, jax.random.fold_in(key, 1))

        assert reg.shape == ()
        assert not jnp.isnan(reg)
        # Should be small for orthonormal features
        assert reg < 1.0

    def test_centered_features(self):
        """Test orthonormal_fro_reg with centered features."""
        key = jax.random.PRNGKey(0)
        x = jax.random.normal(key, (16, 10))
        x_centered = x - x.mean(axis=0)
        reg = orthonormal_fro_reg(x_centered, jax.random.fold_in(key, 1))

        assert reg.shape == ()
        assert not jnp.isnan(reg)

    def test_different_batch_sizes(self):
        """Test orthonormal_fro_reg with different batch sizes."""
        key = jax.random.PRNGKey(0)

        for batch_size in [1, 2, 8, 32]:
            x = jax.random.normal(key, (batch_size, 10))
            reg = orthonormal_fro_reg(x, jax.random.fold_in(key, 1))

            assert reg.shape == ()
            assert not jnp.isnan(reg)

    def test_large_batch(self, large_batch_features):
        """Test orthonormal_fro_reg with large batch size."""
        x, _ = large_batch_features
        key = jax.random.PRNGKey(0)
        reg = orthonormal_fro_reg(x, key)

        assert reg.shape == ()
        assert not jnp.isnan(reg)
        assert not jnp.isinf(reg)

    def test_dtype_preservation(self, sample_features):
        """Test that orthonormal_fro_reg preserves dtype."""
        x, _ = sample_features
        key = jax.random.PRNGKey(0)
        reg = orthonormal_fro_reg(x, key)

        assert reg.dtype == x.dtype

    def test_jit_compilation(self, sample_features):
        """Test that orthonormal_fro_reg can be JIT-compiled."""
        x, _ = sample_features
        key = jax.random.PRNGKey(0)
        jitted_reg = jax.jit(orthonormal_fro_reg)
        reg = jitted_reg(x, key)

        assert reg.shape == ()
        assert not jnp.isnan(reg)

    def test_gradient_computation(self, sample_features):
        """Test gradient computation through orthonormal_fro_reg."""
        x, _ = sample_features
        key = jax.random.PRNGKey(0)

        def reg_fn(x):
            return orthonormal_fro_reg(x, key)

        grad_x = jax.grad(reg_fn)(x)

        assert grad_x.shape == x.shape
        assert not jnp.isnan(grad_x).any()


class TestOrthonormalLogFroReg:
    """Test suite for orthonormal_logfro_reg functional."""

    def test_basic_functionality(self, sample_features):
        """Test basic orthonormal_logfro_reg computation."""
        x, _ = sample_features
        reg = orthonormal_logfro_reg(x)

        assert isinstance(reg, jnp.ndarray)
        assert reg.shape == ()  # Scalar
        assert not jnp.isnan(reg)
        assert not jnp.isinf(reg)

    def test_identity_covariance(self):
        """Test orthonormal_logfro_reg with identity covariance."""
        key = jax.random.PRNGKey(0)
        # Create orthonormal features
        q, _ = jnp.linalg.qr(jax.random.normal(key, (10, 10)))
        x = q  # Already orthonormal
        reg = orthonormal_logfro_reg(x)

        assert reg.shape == ()
        assert not jnp.isnan(reg)
        assert not jnp.isinf(reg)
        # For orthonormal features with identity covariance (eigenvalues â‰ˆ 1),
        # log-Frobenius norm should be small but may not be < 1.0
        assert reg >= 0.0

    def test_centered_features(self):
        """Test orthonormal_logfro_reg with centered features."""
        key = jax.random.PRNGKey(0)
        x = jax.random.normal(key, (16, 10))
        x_centered = x - x.mean(axis=0)
        reg = orthonormal_logfro_reg(x_centered)

        assert reg.shape == ()
        assert not jnp.isnan(reg)

    def test_different_batch_sizes(self):
        """Test orthonormal_logfro_reg with different batch sizes."""
        key = jax.random.PRNGKey(0)

        for batch_size in [1, 2, 8, 32]:
            x = jax.random.normal(key, (batch_size, 10))
            reg = orthonormal_logfro_reg(x)

            assert reg.shape == ()
            assert not jnp.isnan(reg)

    def test_large_batch(self, large_batch_features):
        """Test orthonormal_logfro_reg with large batch size."""
        x, _ = large_batch_features
        reg = orthonormal_logfro_reg(x)

        assert reg.shape == ()
        assert not jnp.isnan(reg)
        assert not jnp.isinf(reg)

    def test_different_feature_dims(self):
        """Test orthonormal_logfro_reg with different feature dimensions."""
        key = jax.random.PRNGKey(0)

        for feat_dim in [5, 10, 50]:
            x = jax.random.normal(key, (16, feat_dim))
            reg = orthonormal_logfro_reg(x)

            assert reg.shape == ()
            assert not jnp.isnan(reg)

    def test_dtype_preservation(self, sample_features):
        """Test that orthonormal_logfro_reg preserves dtype."""
        x, _ = sample_features
        reg = orthonormal_logfro_reg(x)

        assert reg.dtype == x.dtype

    def test_jit_compilation(self, sample_features):
        """Test that orthonormal_logfro_reg can be JIT-compiled."""
        x, _ = sample_features
        jitted_reg = jax.jit(orthonormal_logfro_reg)
        reg = jitted_reg(x)

        assert reg.shape == ()
        assert not jnp.isnan(reg)

    def test_gradient_computation(self, sample_features):
        """Test gradient computation through orthonormal_logfro_reg."""
        x, _ = sample_features

        grad_x = jax.grad(orthonormal_logfro_reg)(x)

        assert grad_x.shape == x.shape
        assert not jnp.isnan(grad_x).any()


class TestLossesIntegration:
    """Integration tests for all JAX loss functions."""

    def test_all_losses_scalar_output(self, sample_features):
        """Test that all losses return scalar outputs."""
        x, y = sample_features
        key = jax.random.PRNGKey(0)
        batch_size, _ = x.shape

        losses = {
            "vamp": vamp_loss(x, y),
            "spectral": spectral_contrastive_loss(x, y),
            "ae": autoencoder_loss(
                x,
                y,
                x,
                jax.random.normal(key, (batch_size, 5)),
                jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 5)),
                y,
            ),
            "fro_reg": orthonormal_fro_reg(x, key),
            "logfro_reg": orthonormal_logfro_reg(x),
        }

        for name, loss in losses.items():
            assert loss.shape == (), f"{name} should return scalar"
            assert not jnp.isnan(loss), f"{name} contains NaN"

    def test_all_losses_jitable(self, sample_features):
        """Test that all losses can be JIT-compiled."""
        x, y = sample_features
        key = jax.random.PRNGKey(0)
        batch_size, _ = x.shape

        loss_fns = {
            "vamp": lambda: vamp_loss(x, y),
            "spectral": lambda: spectral_contrastive_loss(x, y),
            "ae": lambda: autoencoder_loss(
                x,
                y,
                x,
                jax.random.normal(key, (batch_size, 5)),
                jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 5)),
                y,
            ),
            "fro_reg": lambda: orthonormal_fro_reg(x, key),
            "logfro_reg": lambda: orthonormal_logfro_reg(x),
        }

        for name, loss_fn in loss_fns.items():
            jitted_fn = jax.jit(loss_fn)
            loss = jitted_fn()
            assert not jnp.isnan(loss), f"JIT {name} contains NaN"

    def test_all_losses_differentiable(self, sample_features):
        """Test that all losses are differentiable."""
        x, y = sample_features
        key = jax.random.PRNGKey(0)
        _batch_size, _ = x.shape

        # Test loss functions with gradients
        vamp_grad = jax.grad(vamp_loss)(x, y)
        spectral_grad = jax.grad(spectral_contrastive_loss)(x, y)
        fro_grad = jax.grad(lambda x: orthonormal_fro_reg(x, key))(x)
        logfro_grad = jax.grad(orthonormal_logfro_reg)(x)

        for grad, name in [
            (vamp_grad, "vamp"),
            (spectral_grad, "spectral"),
            (fro_grad, "fro_reg"),
            (logfro_grad, "logfro_reg"),
        ]:
            assert not jnp.isnan(grad).any(), f"{name} gradient contains NaN"


class TestEnergyLoss:
    """Test suite for energy_loss functional."""

    def test_default_params(self):
        """Test energy_loss with default parameters."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss = energy_loss(x, y)

        assert isinstance(loss, jnp.ndarray)
        assert loss.shape == ()  # Scalar
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_forward_dtype(self):
        """Test that loss maintains dtype."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim), dtype=jnp.float32)
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim), dtype=jnp.float32)

        loss = energy_loss(x, y)

        assert loss.dtype == x.dtype

    def test_zero_grad_weight(self):
        """Test energy_loss with zero gradient weight."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss = energy_loss(x, y, grad_weight=0.0)

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_large_grad_weight(self):
        """Test energy_loss with large gradient weight."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss = energy_loss(x, y, grad_weight=100.0)

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_gradient_flow(self):
        """Test that gradients flow through energy_loss."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        def loss_fn(x, y):
            return energy_loss(x, y)

        grad_x = jax.grad(loss_fn, argnums=0)(x, y)
        grad_y = jax.grad(loss_fn, argnums=1)(x, y)

        assert not jnp.isnan(grad_x).any()
        assert not jnp.isnan(grad_y).any()

    def test_different_batch_sizes(self):
        """Test energy_loss with different batch sizes."""
        key = jax.random.PRNGKey(42)

        for batch_size in [1, 2, 8, 32]:
            x = jax.random.normal(key, (batch_size, 5))
            y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, 10, 5))
            loss = energy_loss(x, y)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_different_state_dims(self):
        """Test energy_loss with different state dimensions."""
        key = jax.random.PRNGKey(42)

        for state_dim in [2, 5, 10, 20]:
            x = jax.random.normal(key, (16, state_dim))
            y = jax.random.normal(jax.random.fold_in(key, 1), (16, 10, state_dim))
            loss = energy_loss(x, y)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_different_jacobian_dims(self):
        """Test energy_loss with different Jacobian dimensions."""
        key = jax.random.PRNGKey(42)

        for jacobian_dim in [1, 5, 10, 50]:
            x = jax.random.normal(key, (16, 5))
            y = jax.random.normal(jax.random.fold_in(key, 1), (16, jacobian_dim, 5))
            loss = energy_loss(x, y)

            assert loss.shape == ()
            assert not jnp.isnan(loss)

    def test_single_sample(self):
        """Test energy_loss with single sample."""
        key = jax.random.PRNGKey(42)
        x = jax.random.normal(key, (1, 5))
        y = jax.random.normal(jax.random.fold_in(key, 1), (1, 10, 5))

        loss = energy_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_large_batch(self):
        """Test energy_loss with large batch size."""
        key = jax.random.PRNGKey(42)
        batch_size = 128
        state_dim = 10
        jacobian_dim = 50

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss = energy_loss(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_weight_effect_on_loss(self):
        """Test that grad_weight affects the loss value."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss_no_grad = energy_loss(x, y, grad_weight=0.0)
        loss_with_grad = energy_loss(x, y, grad_weight=1e-3)

        # With non-zero grad_weight, loss should generally be different
        assert not jnp.allclose(loss_no_grad, loss_with_grad)

    def test_jacobian_contribution(self):
        """Test that Jacobian term contributes to the loss."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss_fn = lambda w: energy_loss(x, y, grad_weight=w)

        loss = loss_fn(1.0)

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_zero_jacobian(self):
        """Test energy_loss with zero Jacobian."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jnp.zeros((batch_size, jacobian_dim, state_dim))

        loss = energy_loss(x, y, grad_weight=1e-3)

        assert not jnp.isnan(loss)
        assert not jnp.isinf(loss)

    def test_double_precision(self):
        """Test energy_loss with double precision."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim), dtype=jnp.float32)
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim), dtype=jnp.float32)

        loss = energy_loss(x, y)

        assert loss.dtype == jnp.float32
        assert not jnp.isnan(loss)

    def test_deterministic_output(self):
        """Test that energy_loss produces deterministic output for fixed input."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        loss1 = energy_loss(x, y)
        loss2 = energy_loss(x, y)

        assert jnp.allclose(loss1, loss2)

    def test_jit_compilation(self):
        """Test that energy_loss can be JIT compiled."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        jitted_fn = jax.jit(lambda x, y: energy_loss(x, y))
        loss = jitted_fn(x, y)

        assert loss.shape == ()
        assert not jnp.isnan(loss)

    def test_differentiability(self):
        """Test that energy_loss is fully differentiable."""
        key = jax.random.PRNGKey(42)
        batch_size = 16
        state_dim = 5
        jacobian_dim = 10

        x = jax.random.normal(key, (batch_size, state_dim))
        y = jax.random.normal(jax.random.fold_in(key, 1), (batch_size, jacobian_dim, state_dim))

        def loss_fn(x, y):
            return energy_loss(x, y)

        grad_x = jax.grad(loss_fn, argnums=0)(x, y)
        grad_y = jax.grad(loss_fn, argnums=1)(x, y)

        assert not jnp.isnan(grad_x).any()
        assert not jnp.isnan(grad_y).any()
