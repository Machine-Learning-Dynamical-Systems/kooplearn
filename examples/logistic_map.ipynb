{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo-cp5L6mQnh"
   },
   "source": [
    "# Logistic Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwxnFyciguT"
   },
   "source": [
    "> _Authors:_ [Erfan Mirzaei](https://github.com/erfunmirzaei), [Giacomo Turri](https://github.com/g-turri), and [Pietro Novelli](https://pietronvll.github.io/)\n",
    "\n",
    "In this notebook, we reproduce the **Logistic Map** experiment from {footcite:t}Kostic2023DPNets using the kooplearn library.\n",
    "The experiment investigates the challenge of learning representations of the **noisy logistic map**, a one-dimensional dynamical system defined as\n",
    "\n",
    "$$\n",
    "    x_{t + 1} = (rx_{t}(1 - x_{t}) + \\xi_{t}) \\mod 1,\n",
    "$$\n",
    "\n",
    "where $\\xi_t$ is an i.i.d. trigonometric noise term with density $\\propto \\cos^N(x)$ (for even integer $N$), and $r$ is a positive parameter controlling the mapâ€™s dynamics.\n",
    "Here, we set $r = 4$, a case for which the exact solution is known.\n",
    "\n",
    "\n",
    "The corresponding **transfer operator** has rank $N + 1$, is **non-normal**, and admits closed-form eigenvalues and eigenfunctions {footcite:t}Kostic2022. Since $\\mathcal{T}$ is non-normal, learning its spectral decomposition is particularly challenging {footcite:t}Kostic2023SpectralRates.\n",
    "This makes the logistic map a valuable benchmark for testing learned representations. Because the exact form of $\\mathcal{T}$ is available, we can bypass operator regression and focus solely on evaluating the quality of the learned representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrpKkTtV7AeO"
   },
   "source": [
    "To learn the logistic map, we first need to construct the dataset.\n",
    "The process involves the following steps:\n",
    "\n",
    "- Sample from the dynamical system to generate the training, validation, and test trajectories;\n",
    "- Prepare a dataloaders for use with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:08:14.693614Z",
     "start_time": "2024-03-18T15:08:14.690444Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710415185484,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "6CnbIHbOq2p1"
   },
   "outputs": [],
   "source": [
    "# Defining the number of samples for each data split\n",
    "n_train_samples = 10000\n",
    "n_val_samples = 1000\n",
    "n_test_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9cWvQshG1QY"
   },
   "source": [
    "The ``kooplearn`` library provides the function [make_logistic_map](../generated/kooplearn.datasets.make_logistic_map.rst) to generate trajectories from the logistic map.\n",
    "This function takes as input an initial condition, ``X0``, and the number of samples, ``num_samples``, and returns the trajectory starting from ``X0`` and evolving for the specified number of steps.\n",
    "The resulting trajectory therefore has length ``num_samples + 1``.\n",
    "\n",
    "Once the trajectory is generated, we can split it into training, validation, and test subsets for subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:10:22.603462Z",
     "start_time": "2024-03-18T15:10:19.072761Z"
    },
    "executionInfo": {
     "elapsed": 14645,
     "status": "ok",
     "timestamp": 1710415200125,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3jEk8LTTEHiE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "from kooplearn.datasets import make_logistic_map\n",
    "\n",
    "# Defining logistic map \"hyperparameters\" and generating the trajectory\n",
    "M = 20\n",
    "random_state = 42\n",
    "\n",
    "traj = make_logistic_map(\n",
    "    X0=0.5,\n",
    "    n_steps=n_train_samples + n_val_samples + n_test_samples,\n",
    "    M=M,\n",
    "    random_state=0,\n",
    ")  # Setting the random_state for reproducibility\n",
    "\n",
    "dataset = {\n",
    "    \"train\": traj[:n_train_samples],\n",
    "    \"validation\": traj[n_train_samples : n_train_samples + n_val_samples],\n",
    "    \"test\": traj[n_train_samples + n_val_samples :],\n",
    "}\n",
    "\n",
    "train_data = torch.from_numpy(dataset[\"train\"].values).float()\n",
    "val_data = torch.from_numpy(dataset[\"validation\"].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:32:15.804528Z",
     "start_time": "2024-03-18T15:32:15.801159Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "8SdSbmUMj_mo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Creating PyTorch TensorDatasets\n",
    "train_ds = TensorDataset(train_data[:-1], train_data[1:])\n",
    "val_ds = TensorDataset(val_data[:-1], val_data[1:])\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWnw7Ie87pDv"
   },
   "source": [
    "## Learning an appropriate feature map\n",
    "\n",
    "In the next step, we implement the code needed to train the feature maps used to approximate the evolution (transfer) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:08.540941Z",
     "start_time": "2024-03-18T16:04:08.537394Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "AbGr1CoGVALW"
   },
   "outputs": [],
   "source": [
    "# Experiment hyperparameters\n",
    "learning_rate = 2e-4\n",
    "opt = torch.optim.AdamW\n",
    "num_epochs = 50\n",
    "layer_dims = [64, 128, 64]\n",
    "latent_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyCK-fl7oScW"
   },
   "source": [
    "### Sinusoidal Embedding\n",
    "\n",
    "The logistic map is defined on the interval $[0, 1]$ and is inherently periodic.\n",
    "To account for this property, we featurize the signal using trigonometric functions, which naturally encode periodicity.\n",
    "Specifically, we use $\\sin(2\\pi x)$ and $\\cos(2\\pi x)$ as the embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:09.822660Z",
     "start_time": "2024-03-18T16:04:09.819327Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "MKGNFXipHghr"
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is in [0, 1]\n",
    "        x = 2 * torch.pi * x\n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhVkavlKobXE"
   },
   "source": [
    "### Creating an MLP using PyTorch\n",
    "\n",
    "Next, we use the PyTorch library to define our neural network with the specified hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:10.495591Z",
     "start_time": "2024-03-18T16:04:10.490314Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "Iw9ZvcnjGnR2"
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim: int, layer_dims: list[int], activation=torch.nn.LeakyReLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        lin_dims = (\n",
    "            [2] + layer_dims + [latent_dim]\n",
    "        )  # The 2 is for the sinusoidal embedding\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for layer_idx in range(len(lin_dims) - 2):\n",
    "            layers.append(\n",
    "                torch.nn.Linear(\n",
    "                    lin_dims[layer_idx], lin_dims[layer_idx + 1], bias=False\n",
    "                )\n",
    "            )\n",
    "            layers.append(activation())\n",
    "\n",
    "        layers.append(torch.nn.Linear(lin_dims[-2], lin_dims[-1], bias=True))\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.sin_embedding = SinusoidalEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sinusoidal embedding\n",
    "        x = self.sin_embedding(x).float()\n",
    "        # MLP\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kooplearn.linear_model import Ridge\n",
    "from kooplearn.torch.utils import FeatureMapEmbedder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class FeatureMap(torch.nn.Module):\n",
    "    def __init__(self, latent_dim: int, normalize_latents: bool = True):\n",
    "        super().__init__()\n",
    "        self.normalize_latents = normalize_latents\n",
    "        self.backbone = SimpleMLP(latent_dim=latent_dim, layer_dims=layer_dims)\n",
    "        self.lin = torch.nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "\n",
    "    def forward(self, X, lagged: bool = False):\n",
    "        z = self.backbone(X)\n",
    "        if self.normalize_latents:\n",
    "            z = torch.nn.functional.normalize(z, dim=-1)\n",
    "        if lagged:\n",
    "            z = self.lin(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "def train_encoder_only(criterion: torch.nn.Module):\n",
    "    torch.manual_seed(random_state)\n",
    "    # Initialize model, loss and optimizer\n",
    "    model = FeatureMap(latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def step(batch, is_train: bool = True):\n",
    "        batch_X, batch_Y = batch\n",
    "        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        phi_X, phi_Y = model(batch_X), model(batch_Y, lagged=True)\n",
    "        loss = criterion(phi_X, phi_Y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for batch in train_dl:\n",
    "            train_loss.append(step(batch))\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                val_loss.append(step(batch, is_train=False))\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or (epoch == 0):\n",
    "            print(\n",
    "                f\"EPOCH {epoch + 1:>2}  Loss: {np.mean(train_loss):.2f} (train) -  {np.mean(val_loss):.2f} (val)\"\n",
    "            )\n",
    "\n",
    "    embedder = FeatureMapEmbedder(encoder=model)\n",
    "    evolution_operator_model = Ridge(n_components=latent_dim).fit(\n",
    "        embedder.transform(train_data), train_data.numpy(force=True)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model\": evolution_operator_model,\n",
    "        \"feature_map\": embedder.transform,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting VAMPNets\n",
      "EPOCH  1  Loss: -3.23 (train) -  -3.07 (val)\n",
      "EPOCH  5  Loss: -3.29 (train) -  -3.11 (val)\n",
      "EPOCH 10  Loss: -3.30 (train) -  -3.14 (val)\n",
      "EPOCH 15  Loss: -3.31 (train) -  -3.14 (val)\n",
      "EPOCH 20  Loss: -3.30 (train) -  -3.13 (val)\n",
      "EPOCH 25  Loss: -3.20 (train) -  -3.00 (val)\n",
      "EPOCH 30  Loss: -3.10 (train) -  -3.05 (val)\n",
      "EPOCH 35  Loss: -3.00 (train) -  -2.80 (val)\n",
      "EPOCH 40  Loss: -2.95 (train) -  -2.82 (val)\n",
      "EPOCH 45  Loss: -2.95 (train) -  -2.81 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The fitting algorithm discarded 3 dimensions of the 8 requested out of numerical instabilities.\n",
      "The rank attribute has been updated to 5.\n",
      "Consider decreasing the rank parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 50  Loss: -2.94 (train) -  -2.83 (val)\n",
      "Fitting Spectral Contrastive Loss\n",
      "EPOCH  1  Loss: -0.73 (train) -  -1.02 (val)\n",
      "EPOCH  5  Loss: -1.32 (train) -  -1.35 (val)\n",
      "EPOCH 10  Loss: -1.59 (train) -  -1.61 (val)\n",
      "EPOCH 15  Loss: -1.82 (train) -  -1.84 (val)\n",
      "EPOCH 20  Loss: -2.04 (train) -  -2.06 (val)\n",
      "EPOCH 25  Loss: -2.26 (train) -  -2.28 (val)\n",
      "EPOCH 30  Loss: -2.45 (train) -  -2.46 (val)\n",
      "EPOCH 35  Loss: -2.62 (train) -  -2.65 (val)\n",
      "EPOCH 40  Loss: -2.77 (train) -  -2.79 (val)\n",
      "EPOCH 45  Loss: -2.88 (train) -  -2.91 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The fitting algorithm discarded 2 dimensions of the 8 requested out of numerical instabilities.\n",
      "The rank attribute has been updated to 6.\n",
      "Consider decreasing the rank parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 50  Loss: -2.99 (train) -  -3.00 (val)\n"
     ]
    }
   ],
   "source": [
    "from kooplearn.torch.nn import SpectralContrastiveLoss, VampLoss\n",
    "trained_models = {}\n",
    "for name, criterion in zip(\n",
    "    [\"VAMPNets\", \"Spectral Contrastive Loss\"],\n",
    "    [VampLoss(center_covariances=False), SpectralContrastiveLoss()],\n",
    "):\n",
    "    print(f\"Fitting {name}\")\n",
    "    trained_models[name] = train_encoder_only(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determistic feature map based on Chebyshev Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyT:\n",
    "    def __init__(self, feature_dim: int = 8):\n",
    "        self.feature_dim = feature_dim\n",
    "    def __call__(self, x):\n",
    "        vals = [self.scaled_chebyt(n, x) for n in range(self.feature_dim)]\n",
    "        return np.concatenate(vals, axis=-1)\n",
    "    def scaled_chebyt(self, n, x):\n",
    "        return scipy.special.eval_chebyt(n, 2 * x - 1)\n",
    "\n",
    "cheby_model = Ridge(n_components=latent_dim).fit(\n",
    "    ChebyT(feature_dim=latent_dim)(dataset[\"train\"].values))\n",
    "trained_models[\"ChebyT\"] = {\n",
    "    \"model\": cheby_model,\n",
    "    \"feature_map\": ChebyT(feature_dim=latent_dim),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzRBZBFfpkcB"
   },
   "source": [
    "## Evaluation Metrics\n",
    "Now it is time to evaluate the performance of model. To do this, for this specific example we will use two different metrics that are used in the paper: (i) the optimality gap, and (ii) the spectral error.\n",
    "\n",
    "**Optimality Gap**\n",
    "\n",
    "The definition of the optimality gap in the paper is $\n",
    "\\sum_{i = 1}^{3} \\sigma_{i}^{2}(\\tau) - \\mathcal{P}^{0}(w)$, which informs on how close one is to capture the best rank-r approximation of the transfer operator, $\\mathcal{T}$.\n",
    "\n",
    "**Spectral Error**\n",
    "\n",
    "The spectral error is calculated by $ max_i \\ min_j \\left | \\lambda_i(\\mathcal{P}_{\\mathcal{H}}\\tau_{|\\mathcal{H}}) - \\lambda_j(\\tau) \\right |$. This formula measures how well the true eigenvalues of $\\mathcal{T}$ can be recovered within the representation space $\\mathcal{H_w}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PruYq_ppWck"
   },
   "source": [
    "In order to calculate the above metrics for evaluation the performance of the learned representation we should first calculate populational covariances and cross-covariances for the learned feature map.\n",
    "\n",
    "\n",
    "The function `get_population_covs` computes population covariance and cross-covariance matrices relative to a given feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:26.755873Z",
     "start_time": "2024-03-18T16:04:26.749306Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415317409,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "XY8uZGgq7AWg"
   },
   "outputs": [],
   "source": [
    "from kooplearn.datasets import (\n",
    "    compute_logistic_map_eig,\n",
    "    compute_logistic_map_invariant_pdf,\n",
    ")\n",
    "from kooplearn.datasets._logistic_map import TrigonometricNoise, logistic_map\n",
    "\n",
    "invariant_pdf = compute_logistic_map_invariant_pdf(M=M)\n",
    "transition_pdf = TrigonometricNoise(M=M).pdf\n",
    "\n",
    "\n",
    "def get_population_covs(feature_map, integration_points=2**12 + 1):\n",
    "    # Covariance:\n",
    "    x = np.linspace(0, 1, integration_points)\n",
    "    dx = x[1] - x[0]\n",
    "\n",
    "    # Covariance:\n",
    "    phi_X = feature_map(x[:, None])\n",
    "    cov_X = np.einsum(\"xi,xj,x->xij\", phi_X, phi_X, invariant_pdf(x))\n",
    "    cov_X = scipy.integrate.romb(cov_X, dx=dx, axis=0)\n",
    "    # Cross-Covariance:\n",
    "    X, Y = np.meshgrid(x, x, indexing=\"ij\")\n",
    "    cov_XY = np.einsum(\n",
    "        \"xi,yj,x,xy->xyij\",\n",
    "        phi_X,\n",
    "        phi_X,\n",
    "        invariant_pdf(x),\n",
    "        transition_pdf(logistic_map(X) - Y),\n",
    "    )\n",
    "    cov_XY = scipy.integrate.romb(cov_XY, dx=dx, axis=0)\n",
    "    cov_XY = scipy.integrate.romb(cov_XY, dx=dx, axis=0)\n",
    "    return cov_X, cov_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAMPNets - Spectral Dist: 0.2301 - Empirical Spectral Dist: 0.4664\n",
      "Spectral Contrastive Loss - Spectral Dist: 0.1801 - Empirical Spectral Dist: 0.3862\n",
      "ChebyT - Spectral Dist: 0.0789 - Empirical Spectral Dist: 0.2755\n"
     ]
    }
   ],
   "source": [
    "from kooplearn.metrics import directed_hausdorff_distance\n",
    "\n",
    "ref_eigs = compute_logistic_map_eig(M=M)\n",
    "for key, model_dict in trained_models.items():\n",
    "    feature_map = model_dict[\"feature_map\"]\n",
    "    cov, cross_cov = get_population_covs(feature_map)\n",
    "    OLS_eigs = scipy.linalg.eigvals(cross_cov, cov)\n",
    "    empirical_OLS_eigs = model_dict[\"model\"].eig()\n",
    "    spectral_dist = directed_hausdorff_distance(OLS_eigs, ref_eigs)\n",
    "    empirical_spectral_dist = directed_hausdorff_distance(empirical_OLS_eigs, ref_eigs)\n",
    "    print(\n",
    "        f\"{key} - Spectral Dist: {spectral_dist:.4f} - Empirical Spectral Dist: {empirical_spectral_dist:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
