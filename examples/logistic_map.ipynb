{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo-cp5L6mQnh"
   },
   "source": [
    "# Logistic Map Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIwxnFyciguT"
   },
   "source": [
    "> _Authors:_ [Erfan Mirzaei](https://github.com/erfunmirzaei), [Giacomo Turri](https://github.com/g-turri), and [Pietro Novelli](https://pietronvll.github.io/)\n",
    "\n",
    "In this notebook, we reproduce the **Logistic Map** experiment from {footcite:t}Kostic2023DPNets using the kooplearn library.\n",
    "The experiment investigates the challenge of learning representations of the **noisy logistic map**, a one-dimensional dynamical system defined as\n",
    "\n",
    "$$\n",
    "    x_{t + 1} = (rx_{t}(1 - x_{t}) + \\xi_{t}) \\mod 1,\n",
    "$$\n",
    "\n",
    "where $\\xi_t$ is an i.i.d. trigonometric noise term with density $\\propto \\cos^N(x)$ (for even integer $N$), and $r$ is a positive parameter controlling the map’s dynamics.\n",
    "Here, we set $r = 4$, a case for which the exact solution is known.\n",
    "\n",
    "\n",
    "The corresponding **transfer operator** has rank $N + 1$, is **non-normal**, and admits closed-form eigenvalues and eigenfunctions {footcite:t}Kostic2022. Since $\\mathcal{T}$ is non-normal, learning its spectral decomposition is particularly challenging {footcite:t}Kostic2023SpectralRates.\n",
    "This makes the logistic map a valuable benchmark for testing learned representations. Because the exact form of $\\mathcal{T}$ is available, we can bypass operator regression and focus solely on evaluating the quality of the learned representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T14:59:30.502885Z",
     "start_time": "2024-03-18T14:59:30.499996Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710420134492,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "bKH7k0uaq9a7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrpKkTtV7AeO"
   },
   "source": [
    "### Building the Dataset\n",
    "\n",
    "To learn the logistic map, we first need to construct the dataset.\n",
    "The process involves the following steps:\n",
    "\n",
    "- Sample from the dynamical system to generate the training, validation, and test trajectories;\n",
    "- Prepare a dataloaders for use with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:08:14.693614Z",
     "start_time": "2024-03-18T15:08:14.690444Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710415185484,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "6CnbIHbOq2p1"
   },
   "outputs": [],
   "source": [
    "# Defining the number of samples for each data split\n",
    "n_train_samples = 10000\n",
    "n_val_samples = 1000\n",
    "n_test_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9cWvQshG1QY"
   },
   "source": [
    "The ``kooplearn`` library provides the function [make_logistic_map](../generated/kooplearn.datasets.make_logistic_map.rst) to generate trajectories from the logistic map.\n",
    "This function takes as input an initial condition, ``X0``, and the number of samples, ``num_samples``, and returns the trajectory starting from ``X0`` and evolving for the specified number of steps.\n",
    "The resulting trajectory therefore has length ``num_samples + 1``.\n",
    "\n",
    "Once the trajectory is generated, we can split it into training, validation, and test subsets for subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:10:22.603462Z",
     "start_time": "2024-03-18T15:10:19.072761Z"
    },
    "executionInfo": {
     "elapsed": 14645,
     "status": "ok",
     "timestamp": 1710415200125,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3jEk8LTTEHiE"
   },
   "outputs": [],
   "source": [
    "from kooplearn.datasets import make_logistic_map\n",
    "\n",
    "traj = make_logistic_map(\n",
    "    X0 = 0.5,\n",
    "    n_steps = n_train_samples + n_val_samples + n_test_samples,\n",
    "    M = 20,\n",
    "    random_state = 0) # Setting the random_state for reproducibility\n",
    "\n",
    "dataset = {\n",
    "    \"train\": traj[: n_train_samples],\n",
    "    \"validation\": traj[n_train_samples : n_train_samples + n_val_samples],\n",
    "    \"test\": traj[n_train_samples + n_val_samples :],\n",
    "}\n",
    "\n",
    "train_data = torch.from_numpy(dataset[\"train\"].values).float()\n",
    "val_data = torch.from_numpy(dataset[\"validation\"].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T15:32:15.804528Z",
     "start_time": "2024-03-18T15:32:15.801159Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "8SdSbmUMj_mo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "# Creating PyTorch TensorDatasets\n",
    "train_ds = TensorDataset(train_data[:-1], train_data[1:])\n",
    "val_ds = TensorDataset(val_data[:-1], val_data[1:])\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWnw7Ie87pDv"
   },
   "source": [
    "## Evolution operator training algorithms\n",
    "\n",
    "In the next step, we implement the code needed to train the feature maps used to approximate the evolution (transfer) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:08.540941Z",
     "start_time": "2024-03-18T16:04:08.537394Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "AbGr1CoGVALW"
   },
   "outputs": [],
   "source": [
    "# Init report dict\n",
    "dl_reports = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Experiment hyperparameters\n",
    "learning_rate = 1e-3\n",
    "opt = torch.optim.Adam\n",
    "num_epochs = 100\n",
    "random_state = 42\n",
    "layer_dims = [64, 128, 64]\n",
    "metric_deformation_coeff = 1\n",
    "latent_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyCK-fl7oScW"
   },
   "source": [
    "### Sinusoidal Embedding\n",
    "\n",
    "The logistic map is defined on the interval $[0, 1]$ and is inherently periodic.\n",
    "To account for this property, we featurize the signal using trigonometric functions, which naturally encode periodicity.\n",
    "Specifically, we use $\\sin(2\\pi x)$ and $\\cos(2\\pi x)$ as the embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:09.822660Z",
     "start_time": "2024-03-18T16:04:09.819327Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "MKGNFXipHghr"
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is in [0, 1]\n",
    "        x = 2 * torch.pi * x\n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhVkavlKobXE"
   },
   "source": [
    "### Creating an MLP using PyTorch\n",
    "\n",
    "Next, we use the PyTorch library to define our neural network with the specified hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:10.495591Z",
     "start_time": "2024-03-18T16:04:10.490314Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415202807,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "Iw9ZvcnjGnR2"
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, latent_dim: int, layer_dims: list[int], activation=torch.nn.LeakyReLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        lin_dims = ([2] + layer_dims + [latent_dim])  # The 2 is for the sinusoidal embedding\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for layer_idx in range(len(lin_dims) - 2):\n",
    "            layers.append(torch.nn.Linear(lin_dims[layer_idx], lin_dims[layer_idx + 1], bias=False))\n",
    "            layers.append(activation())\n",
    "\n",
    "        layers.append(torch.nn.Linear(lin_dims[-2], lin_dims[-1], bias=True))\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.sin_embedding = SinusoidalEmbedding()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sinusoidal embedding\n",
    "        x = self.sin_embedding(x).float()\n",
    "        # MLP\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OACmEQzGp2Wp"
   },
   "source": [
    "### Defining the ``FeatureMap``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kooplearn.torch.utils import FeatureMapEmbedder\n",
    "from kooplearn.linear_model import Ridge\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class FeatureMap(torch.nn.Module):\n",
    "    def __init__(self, latent_dim: int, normalize_latents: bool = True):\n",
    "        super().__init__()\n",
    "        self.normalize_latents = normalize_latents\n",
    "        self.backbone = SimpleMLP(latent_dim=latent_dim, layer_dims=layer_dims)\n",
    "        self.lin = torch.nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "    \n",
    "    def forward(self, X, lagged:bool=False):\n",
    "        z = self.backbone(X)\n",
    "        if self.normalize_latents:\n",
    "            z = torch.nn.functional.normalize(z, dim=-1)\n",
    "        if lagged:\n",
    "            z = self.lin(z)\n",
    "        return z\n",
    "    \n",
    "def train_encoder_only(criterion: torch.nn.Module):\n",
    "    torch.manual_seed(random_state)\n",
    "    # Initialize model, loss and optimizer\n",
    "    model = FeatureMap(latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def step(batch, is_train:bool = True):\n",
    "        batch_X, batch_Y = batch\n",
    "        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device) \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        phi_X, phi_Y = model(batch_X), model(batch_Y, lagged=True)\n",
    "        loss = criterion(phi_X, phi_Y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for batch in train_dl:\n",
    "            train_loss.append(step(batch))\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                val_loss.append(step(batch, is_train=False))\n",
    "\n",
    "        if (epoch + 1)%5 == 0 or (epoch == 0):\n",
    "            print(f\"EPOCH {epoch + 1:>2}  Loss: {np.mean(train_loss):.2f} (train) -  {np.mean(val_loss):.2f} (val)\")\n",
    "    \n",
    "    embedder = FeatureMapEmbedder(encoder=model)\n",
    "    evolution_operator_model = Ridge(n_components=latent_dim).fit(embedder.transform(train_data), train_data.numpy(force=True))\n",
    "\n",
    "    return {\n",
    "        \"model\": evolution_operator_model,\n",
    "        \"embedder\": embedder,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting VAMPNets\n",
      "EPOCH  1  Loss: -3.19 (train) -  -2.99 (val)\n",
      "EPOCH  5  Loss: -3.40 (train) -  -3.02 (val)\n",
      "EPOCH 10  Loss: -2.75 (train) -  -2.87 (val)\n",
      "EPOCH 15  Loss: -2.61 (train) -  -2.62 (val)\n",
      "EPOCH 20  Loss: -2.61 (train) -  -2.38 (val)\n",
      "EPOCH 25  Loss: -2.50 (train) -  -2.44 (val)\n",
      "EPOCH 30  Loss: -2.37 (train) -  -1.92 (val)\n",
      "EPOCH 35  Loss: -2.83 (train) -  -2.82 (val)\n",
      "EPOCH 40  Loss: -2.53 (train) -  -2.90 (val)\n",
      "EPOCH 45  Loss: -2.39 (train) -  -2.45 (val)\n",
      "EPOCH 50  Loss: -2.58 (train) -  -2.85 (val)\n",
      "EPOCH 55  Loss: -2.44 (train) -  -2.88 (val)\n",
      "EPOCH 60  Loss: -2.31 (train) -  -2.30 (val)\n",
      "EPOCH 65  Loss: -2.37 (train) -  -2.34 (val)\n",
      "EPOCH 70  Loss: -2.40 (train) -  -2.32 (val)\n",
      "EPOCH 75  Loss: -2.40 (train) -  -2.40 (val)\n",
      "EPOCH 80  Loss: -2.50 (train) -  -2.32 (val)\n",
      "EPOCH 85  Loss: -2.41 (train) -  -2.60 (val)\n",
      "EPOCH 90  Loss: -2.40 (train) -  -2.48 (val)\n",
      "EPOCH 95  Loss: -2.37 (train) -  -2.01 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The fitting algorithm discarded 3 dimensions of the 8 requested out of numerical instabilities.\n",
      "The rank attribute has been updated to 5.\n",
      "Consider decreasing the rank parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 100  Loss: -2.50 (train) -  -2.48 (val)\n",
      "Fitting Spectral Contrastive Loss\n",
      "EPOCH  1  Loss: -0.99 (train) -  -1.23 (val)\n",
      "EPOCH  5  Loss: -2.08 (train) -  -2.19 (val)\n",
      "EPOCH 10  Loss: -2.87 (train) -  -2.92 (val)\n",
      "EPOCH 15  Loss: -3.20 (train) -  -3.23 (val)\n",
      "EPOCH 20  Loss: -3.32 (train) -  -3.33 (val)\n",
      "EPOCH 25  Loss: -3.45 (train) -  -3.49 (val)\n",
      "EPOCH 30  Loss: -3.73 (train) -  -3.79 (val)\n",
      "EPOCH 35  Loss: -4.00 (train) -  -4.05 (val)\n",
      "EPOCH 40  Loss: -4.20 (train) -  -4.23 (val)\n",
      "EPOCH 45  Loss: -4.35 (train) -  -4.37 (val)\n",
      "EPOCH 50  Loss: -4.43 (train) -  -4.43 (val)\n",
      "EPOCH 55  Loss: -4.43 (train) -  -4.43 (val)\n",
      "EPOCH 60  Loss: -4.45 (train) -  -4.45 (val)\n",
      "EPOCH 65  Loss: -4.48 (train) -  -4.50 (val)\n",
      "EPOCH 70  Loss: -4.48 (train) -  -4.48 (val)\n",
      "EPOCH 75  Loss: -4.48 (train) -  -4.44 (val)\n",
      "EPOCH 80  Loss: -4.49 (train) -  -4.48 (val)\n",
      "EPOCH 85  Loss: -4.51 (train) -  -4.47 (val)\n",
      "EPOCH 90  Loss: -4.48 (train) -  -4.48 (val)\n",
      "EPOCH 95  Loss: -4.51 (train) -  -4.49 (val)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The fitting algorithm discarded 1 dimensions of the 8 requested out of numerical instabilities.\n",
      "The rank attribute has been updated to 7.\n",
      "Consider decreasing the rank parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 100  Loss: -4.53 (train) -  -4.49 (val)\n"
     ]
    }
   ],
   "source": [
    "from kooplearn.torch.nn import VampLoss, SpectralContrastiveLoss\n",
    "\n",
    "for name, criterion in zip([\"VAMPNets\", \"Spectral Contrastive Loss\"], [ VampLoss(center_covariances=False), SpectralContrastiveLoss()]):\n",
    "    print(f\"Fitting {name}\")\n",
    "    trained_models[name] = train_encoder_only(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzRBZBFfpkcB"
   },
   "source": [
    "## Evaluation Metrics\n",
    "Now it is time to evaluate the performance of model. To do this, for this specific example we will use two different metrics that are used in the paper: (i) the optimality gap, and (ii) the spectral error.\n",
    "\n",
    "**Optimality Gap**\n",
    "\n",
    "The definition of the optimality gap in the paper is $\n",
    "\\sum_{i = 1}^{3} \\sigma_{i}^{2}(\\tau) - \\mathcal{P}^{0}(w)$, which informs on how close one is to capture the best rank-r approximation of the transfer operator, $\\mathcal{T}$.\n",
    "\n",
    "**Spectral Error**\n",
    "\n",
    "The spectral error is calculated by $ max_i \\ min_j \\left | \\lambda_i(\\mathcal{P}_{\\mathcal{H}}\\tau_{|\\mathcal{H}}) - \\lambda_j(\\tau) \\right |$. This formula measures how well the true eigenbalues of $\\mathcal{T}$ can be recovered within the representation space $\\mathcal{H_w}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PruYq_ppWck"
   },
   "source": [
    "In order to calculate the above metrics for evaluation the performance of the learned representation we should first calculate populational covariances and cross-covariances for the learned feature map.\n",
    "\n",
    "\n",
    "The population_covs function computes population covariance and cross-covariance matrices based on given feature and logistic maps. It begins by generating input data points and then calculates eigenvalues and left eigenfunctions of the logistic map. Subsequently, it normalizes the Perron eigenvector and evaluates the feature map on the input data to obtain a feature matrix. Covariance and cross-covariance matrices are then computed using the feature matrix and normalized Perron eigenvector.\n",
    "\n",
    "On the other hand, the evaluate_representation function assesses the performance of the obtained representation. It computes the OLS estimator, estimates eigenvalues, and evaluates metrics such as the optimality gap and spectral error to provide insights into the model's quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:26.755873Z",
     "start_time": "2024-03-18T16:04:26.749306Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415317409,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "XY8uZGgq7AWg"
   },
   "outputs": [],
   "source": [
    "from kooplearn.abc import FeatureMap\n",
    "from scipy.integrate import romb\n",
    "\n",
    "def population_covs(feature_map: FeatureMap, logistic: LogisticMap, pow_of_two_k: int = 12):\n",
    "    \"\"\"Computes the population covariance and cross-covariance\"\"\"\n",
    "    x = np.linspace(0, 1, 2**pow_of_two_k + 1)[:, None]\n",
    "    vals, lv = logistic.eig(eval_left_on=x) # returns eigenvalues and left eigenfunctions\n",
    "    perron_eig_idx = np.argmax(np.abs(vals))\n",
    "    pi = lv[:, perron_eig_idx] #eigenfunction correspond with the max eigenvalue\n",
    "    assert np.isreal(pi).all()\n",
    "    pi = pi.real\n",
    "    pi = pi / romb(pi, dx=1 / 2**pow_of_two_k)  # Normalization of π\n",
    "    # Evaluating the feature map\n",
    "    phi = feature_map(x)  # [2**pow_of_two_k + 1, d]\n",
    "    # Covariance\n",
    "    cov_unfolded = (\n",
    "        phi.reshape(2**pow_of_two_k + 1, -1, 1)\n",
    "        * phi.reshape(2**pow_of_two_k + 1, 1, -1)\n",
    "        * pi.reshape(-1, 1, 1)\n",
    "    )\n",
    "    cov = romb(cov_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "    # Cross-covariance\n",
    "    alphas = np.stack(\n",
    "        [logistic.noise_feature_composed_map(x, n) for n in range(logistic.N + 1)],\n",
    "        axis=1,\n",
    "    )\n",
    "    betas = np.stack(\n",
    "        [logistic.noise_feature(x, n) for n in range(logistic.N + 1)], axis=1\n",
    "    )\n",
    "\n",
    "    cov_alpha_unfolded = (\n",
    "        phi.reshape(2**pow_of_two_k + 1, -1, 1)\n",
    "        * alphas.reshape(2**pow_of_two_k + 1, 1, -1)\n",
    "        * pi.reshape(-1, 1, 1)\n",
    "    )\n",
    "    cov_beta_unfolded = phi.reshape(2**pow_of_two_k + 1, -1, 1) * betas.reshape(\n",
    "        2**pow_of_two_k + 1, 1, -1\n",
    "    )\n",
    "\n",
    "    cov_alpha = romb(cov_alpha_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "    cov_beta = romb(cov_beta_unfolded, dx=1 / 2**pow_of_two_k, axis=0)\n",
    "\n",
    "    cross_cov = cov_alpha @ (cov_beta.T)\n",
    "    return cov, cross_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:27.193581Z",
     "start_time": "2024-03-18T16:04:27.188757Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710415320580,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "3N2IkYQZ6ISP"
   },
   "outputs": [],
   "source": [
    "from kooplearn._src.metrics import directed_hausdorff_distance\n",
    "from kooplearn._src.utils import topk\n",
    "\n",
    "def evaluate_representation(feature_map: FeatureMap, logistic_map: LogisticMap):\n",
    "    report = {}\n",
    "    # Compute OLS estimator\n",
    "    cov, cross_cov = population_covs(feature_map, logistic_map)\n",
    "    OLS_estimator = np.linalg.solve(cov, cross_cov)\n",
    "\n",
    "    # Eigenvalue estimation\n",
    "    OLS_eigs = np.linalg.eigvals(OLS_estimator)\n",
    "    top_eigs = topk(np.abs(OLS_eigs), 3)\n",
    "    OLS_eigs = OLS_eigs[top_eigs.indices]\n",
    "\n",
    "    true_eigs = logistic_map.eig()\n",
    "    top_eigs = topk(np.abs(true_eigs), 3)\n",
    "    true_eigs = true_eigs[top_eigs.indices]\n",
    "\n",
    "    report[\"hausdorff-distance\"] = directed_hausdorff_distance(OLS_eigs, true_eigs)\n",
    "    # VAMP2-score\n",
    "    M = np.linalg.multi_dot(\n",
    "        [\n",
    "            np.linalg.pinv(cov, hermitian=True),\n",
    "            cross_cov,\n",
    "            np.linalg.pinv(cov, hermitian=True),\n",
    "            cross_cov.T,\n",
    "        ]\n",
    "    )\n",
    "    feature_dim = cov.shape[0]\n",
    "    report[\"optimality-gap\"] = np.sum(logistic_map.svals()[:feature_dim] ** 2) - np.trace(M)\n",
    "\n",
    "    # Feasibility\n",
    "    # report[\"feasibility-gap\"] = np.linalg.norm(cov - np.eye(feature_dim), ord=2)\n",
    "    # report[\"estimator-eigenvalues\"] = OLS_eigs\n",
    "    # report[\"covariance-eigenvalues\"] = np.linalg.eigvalsh(cov)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzGMOWhVr0SR"
   },
   "source": [
    "Now we can assess the performace of the learned representation based on optimality gap and spectral error or hausdorff distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:04:30.048810Z",
     "start_time": "2024-03-18T16:04:29.944559Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1710415330683,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "91WBcVsRrztw",
    "outputId": "6c7f0b9e-3219-4709-f455-b5b9bbf256c7"
   },
   "outputs": [],
   "source": [
    "evaluate_representation(dpnet_fmap, logmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QegCVjdyrzbX"
   },
   "source": [
    "As we saw the result is aligned with the reported results in the paper, however we only run for one seed. Therefore, to establish a more meaningful statistical comparison between different models which tries to learn the representation we run each models for 20 independent runs and report mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slYYDYwmqXvR"
   },
   "source": [
    "## Evaluating DPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt0QAfHzu0Dk"
   },
   "source": [
    "Now we run the DPNet over 20 independent runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:07:36.606017Z",
     "start_time": "2024-03-18T16:05:03.458155Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1286092,
     "status": "ok",
     "timestamp": 1710417204771,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "drxXOF6gYvUV",
    "outputId": "e29f801a-45f6-4550-d026-67bb958c011d"
   },
   "outputs": [],
   "source": [
    "relaxed = False\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    dpnet_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        DPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = DPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(dpnet_fmap.lightning_module.encoder)\n",
    "    kaiming_init(dpnet_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    dpnet_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(dpnet_fmap, logmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRd8x0k9uihM"
   },
   "source": [
    "Now we calculate mean and standard deviation for each metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:22.823847Z",
     "start_time": "2024-03-18T16:11:22.819913Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710417242774,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "pVoxJYT76UVF"
   },
   "outputs": [],
   "source": [
    "def stack_reports(reports):\n",
    "    stacked = {}\n",
    "    for key in [\n",
    "        \"hausdorff-distance\",\n",
    "        \"optimality-gap\",\n",
    "        # \"feasibility-gap\",\n",
    "        # \"fit-time\",\n",
    "        # \"time_per_epoch\",\n",
    "    ]:\n",
    "        if key in reports[0]:\n",
    "            stacked[key] = np.mean([r[key] for r in reports])\n",
    "            key_std = key + \"_std\"\n",
    "            stacked[key_std] = np.std([r[key] for r in reports])\n",
    "    # for key in [\"estimator-eigenvalues\", \"covariance-eigenvalues\"]:\n",
    "    #     stacked[key] = np.stack([r[key] for r in reports])\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:23.881292Z",
     "start_time": "2024-03-18T16:11:23.878470Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710417242774,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "NA7coxb1uhsK"
   },
   "outputs": [],
   "source": [
    "full_DPNet_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:11:25.231580Z",
     "start_time": "2024-03-18T16:11:25.227901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1710417264732,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "x6H5D8NdAex1",
    "outputId": "f868d009-cf17-4664-e620-0ab448bcc323"
   },
   "outputs": [],
   "source": [
    "full_DPNet_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA3C9Aar2pZM"
   },
   "source": [
    "## Evaluating DPNet relaxed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqlCahty2pZS"
   },
   "source": [
    "Now we run the DPNet-relaxed over 20 independent runs. To do so we just need to change the flag that we pass to the DPNet module to whether use the relaxed version or not. For more information about the difference between the relaxed version of loss function with the original read the section 3 of the paper \\cite{}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:15:28.102586Z",
     "start_time": "2024-03-18T16:12:56.603527Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1967449,
     "status": "ok",
     "timestamp": 1710420130814,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "WEGvBWHz2pZS",
    "outputId": "8fcce119-3076-4199-db67-f687c5384ef1"
   },
   "outputs": [],
   "source": [
    "DPNets = {'loss_fn': DPLoss, 'loss_kwargs': {'relaxed': True, 'metric_deformation': metric_deformation_coeff, 'center_covariances': False}}\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    dpnet_relaxed_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        DPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = DPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(dpnet_relaxed_fmap.lightning_module.encoder)\n",
    "    kaiming_init(dpnet_relaxed_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    dpnet_relaxed_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(dpnet_relaxed_fmap, logmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:22:21.767884Z",
     "start_time": "2024-03-18T16:22:21.764713Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710420130815,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "nRh1Dnzs2pZS"
   },
   "outputs": [],
   "source": [
    "full_DPNet_relaxed_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:22:22.653803Z",
     "start_time": "2024-03-18T16:22:22.650129Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710420130815,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "IRwc_QW82pZS",
    "outputId": "05a077b8-b6dc-4c3e-b5f5-93ae5fbf9ace"
   },
   "outputs": [],
   "source": [
    "full_DPNet_relaxed_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKzufK-C3j_x"
   },
   "source": [
    "## Evaluating VAMPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Frg-JO6h3j_x"
   },
   "source": [
    "Now we run the VAMPNet \\cite{} over 20 independent runs. To do so we just need to import the VAMPNet class from the kooplearn's feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:26:16.341495Z",
     "start_time": "2024-03-18T16:26:16.338806Z"
    },
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1710420206199,
     "user": {
      "displayName": "Erfan Mirzaei",
      "userId": "05133291337321121373"
     },
     "user_tz": -210
    },
    "id": "ID4wDzbz4-Mo"
   },
   "outputs": [],
   "source": [
    "from kooplearn.nn import VAMPLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:28:44.980486Z",
     "start_time": "2024-03-18T16:26:16.770848Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4v-mbRn3j_x"
   },
   "outputs": [],
   "source": [
    "VAMPNets = {'loss_fn': VAMPLoss, 'loss_kwargs': {'schatten_norm': 2, 'center_covariances': False}}\n",
    "report = []\n",
    "\n",
    "for rng_seed in range(num_rng_seeds):\n",
    "\n",
    "    trainer = lightning.Trainer(**trainer_kwargs)\n",
    "    net_kwargs = {\"feature_dim\": feature_dim, \"layer_dims\": layer_dims}\n",
    "\n",
    "    # Defining the model\n",
    "    vampnet_fmap = NNFeatureMap(\n",
    "        SimpleMLP,\n",
    "        VAMPNets['loss_fn'],\n",
    "        opt,\n",
    "        trainer,\n",
    "        encoder_kwargs = net_kwargs,\n",
    "        loss_kwargs = VAMPNets['loss_kwargs'],\n",
    "        optimizer_kwargs = {\"lr\": 5e-5},\n",
    "        lagged_encoder = SimpleMLP,\n",
    "        lagged_encoder_kwargs = net_kwargs,\n",
    "        seed = rng_seed)\n",
    "\n",
    "    # Init\n",
    "    torch.manual_seed(rng_seed)\n",
    "    kaiming_init(vampnet_fmap.lightning_module.encoder)\n",
    "    kaiming_init(vampnet_fmap.lightning_module.lagged_encoder)\n",
    "\n",
    "    vampnet_fmap.fit(train_dl)\n",
    "\n",
    "    report.append(evaluate_representation(vampnet_fmap, logmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:36.613994Z",
     "start_time": "2024-03-18T16:31:36.610924Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "WjI9vyct3j_y"
   },
   "outputs": [],
   "source": [
    "full_VAMPNet_report = stack_reports(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:37.383407Z",
     "start_time": "2024-03-18T16:31:37.379939Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "5veUFzO13j_y"
   },
   "outputs": [],
   "source": [
    "full_VAMPNet_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBI5eneg7tEJ"
   },
   "source": [
    "## Evaluating Noise Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:44.539383Z",
     "start_time": "2024-03-18T16:31:44.536826Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "kSmZolYS80CF"
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "from kooplearn.models.feature_maps import ConcatenateFeatureMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:31:54.956597Z",
     "start_time": "2024-03-18T16:31:54.953001Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "9tSyPOpz8DhJ"
   },
   "outputs": [],
   "source": [
    "def noise_feat(n, x):\n",
    "    return logmap.noise_feature(x, n)\n",
    "\n",
    "def NoiseKernel(order: int = 3):\n",
    "    binom_coeffs = [scipy.special.binom(N_exp, i) for i in range(N_exp + 1)]\n",
    "    sorted_coeffs = np.argsort(binom_coeffs)\n",
    "\n",
    "    fn_list = [partial(noise_feat, n) for n in sorted_coeffs[:order]]\n",
    "\n",
    "    return ConcatenateFeatureMaps(fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:46.565758Z",
     "start_time": "2024-03-18T16:32:46.501398Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "68y28MYY-xA4"
   },
   "outputs": [],
   "source": [
    "full_NoiseKernel_report = evaluate_representation(NoiseKernel(feature_dim), logistic_map=logmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:47.849959Z",
     "start_time": "2024-03-18T16:32:47.846465Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "2bWL8K-8-sNl"
   },
   "outputs": [],
   "source": [
    "full_NoiseKernel_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4j-uK_W_mW7"
   },
   "source": [
    "## Evaluating Cheby-T Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:54.930220Z",
     "start_time": "2024-03-18T16:32:54.926897Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "-70rxpVE_rVa"
   },
   "outputs": [],
   "source": [
    "def scaled_chebyt(n, x):\n",
    "    return scipy.special.eval_chebyt(n, 2 * x - 1)\n",
    "\n",
    "def ChebyT(feature_dim: int = 3):\n",
    "\n",
    "    fn_list = [partial(scaled_chebyt, n) for n in range(feature_dim)]\n",
    "    return ConcatenateFeatureMaps(fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:32:59.542982Z",
     "start_time": "2024-03-18T16:32:59.483130Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "UxadJZCu_zSD"
   },
   "outputs": [],
   "source": [
    "full_ChebyTKenel_report = evaluate_representation(ChebyT(feature_dim),logmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:33:00.087502Z",
     "start_time": "2024-03-18T16:33:00.083998Z"
    },
    "colab": {
     "background_save": true
    },
    "id": "_MUnVAPU_4_3"
   },
   "outputs": [],
   "source": [
    "full_ChebyTKenel_report"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
