{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing VAMPNets on the Ordered MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configs\n",
    "import ml_confs as mlcfg\n",
    "configs = mlcfg.from_file('configs.yaml', register_jax_pytree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pipeline\n",
    "from datasets import load_from_disk\n",
    "ordered_MNIST = load_from_disk('__data__')\n",
    "#Creating a copy of the dataset in numpy format\n",
    "np_ordered_MNIST = ordered_MNIST.with_format(type='numpy', columns=['image', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "#Setting up the architecture\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output num_classes classes\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, num_classes)\n",
    "        )  \n",
    "        torch.nn.init.orthogonal_(self.out[0].weight)      \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1) # add channel dimension if needed\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import lightning\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from kooplearn.abc import TrainableFeatureMap\n",
    "from typing import Optional\n",
    "\n",
    "#Following kooplearn implementations, we define a Pytorch Lightning module\n",
    "class ClassifierModule(lightning.LightningModule):\n",
    "    def __init__(self, num_classes: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = CNNEncoder(num_classes=num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        output = self.encoder(images)               \n",
    "        loss = self.loss_fn(output, labels)\n",
    "        with torch.no_grad():\n",
    "            pred_labels = output.argmax(dim=1)\n",
    "            accuracy = (pred_labels == labels).float().mean()\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        output = self.encoder(images)  \n",
    "        pred_labels = output.argmax(dim=1)\n",
    "        accuracy = (pred_labels == labels).float().mean() # Scalar\n",
    "        return {'accuracy': accuracy}\n",
    "    \n",
    "class ClassifierFeatureMap(TrainableFeatureMap):\n",
    "    def __init__(\n",
    "                self, \n",
    "                num_classes: int,\n",
    "                learning_rate: float,\n",
    "                trainer_kwargs: dict,\n",
    "                seed: Optional[int] = None  \n",
    "                ):\n",
    "        #Set rng seed\n",
    "        lightning.seed_everything(seed)\n",
    "        self.seed = seed\n",
    "        self._lightning_module = ClassifierModule(num_classes, learning_rate)\n",
    "        \n",
    "        #Init trainer\n",
    "        self.trainer = lightning.Trainer(**trainer_kwargs)\n",
    "        \n",
    "        self._storage = {\n",
    "            'num_classes': num_classes,\n",
    "            'learning_rate': learning_rate,\n",
    "            'trainer_kwargs': trainer_kwargs,\n",
    "            'seed': seed\n",
    "        }\n",
    "        self._trainer_kwargs = trainer_kwargs\n",
    "        self._is_fitted = False\n",
    "        \n",
    "    @property\n",
    "    def is_fitted(self) -> bool:\n",
    "        return self._is_fitted\n",
    "    \n",
    "    @property\n",
    "    def lookback_len(self) -> int:\n",
    "        return 1 #Hardcoding it here, as we are not using lookback windows\n",
    "    \n",
    "    def save(self, path: os.PathLike):\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        ckpt = path / 'weights.ckpt'\n",
    "        params = path / 'params.pkl'\n",
    "        self.trainer.save_checkpoint(str(ckpt))\n",
    "        self._storage['is_fitted'] = self._is_fitted\n",
    "        with open(params, 'wb') as f:\n",
    "            pickle.dump(self._storage, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: os.PathLike):\n",
    "        with open(Path(path) / 'params.pkl', 'rb') as f:\n",
    "            storage = pickle.load(f)\n",
    "        _is_fitted = storage.pop('is_fitted')\n",
    "        feature_map = cls(**storage)\n",
    "        feature_map._is_fitted = _is_fitted\n",
    "        ckpt = Path(path) / 'weights.ckpt'\n",
    "        feature_map._lightning_module = ClassifierModule.load_from_checkpoint(str(ckpt))\n",
    "        return feature_map\n",
    "\n",
    "    def fit(self, **trainer_fit_kwargs: dict):\n",
    "        if 'model' in trainer_fit_kwargs:\n",
    "            logging.warn(\"The 'model' keyword should not be specified in trainer_fit_kwargs. The model is automatically set to the DPNet feature map, and the provided model is ignored.\")\n",
    "            trainer_fit_kwargs = trainer_fit_kwargs.copy()\n",
    "            del trainer_fit_kwargs['model']\n",
    "        self.trainer.fit(model=self._lightning_module, **trainer_fit_kwargs)\n",
    "        self._is_fitted = True\n",
    "\n",
    "    def __call__(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X.to(self._lightning_module.device)\n",
    "        self._lightning_module.eval()\n",
    "        with torch.no_grad():\n",
    "            embedded_X = self._lightning_module.encoder(X)\n",
    "            embedded_X = embedded_X.detach().numpy()\n",
    "        return embedded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(ordered_MNIST['train'], batch_size=configs.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(ordered_MNIST['validation'], batch_size=len(ordered_MNIST['validation']), shuffle=False)\n",
    "\n",
    "#Metrics Logging\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_acc = []\n",
    "        self.train_steps = []\n",
    "        self.val_acc = []\n",
    "        self.val_steps = []\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        # Get the metrics from the last training step\n",
    "        self.train_acc.append(outputs['accuracy'].item())\n",
    "        self.train_steps.append(trainer.global_step)\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "        # Get the metrics from the last validation step\n",
    "        self.val_acc.append(outputs['accuracy'].item())\n",
    "        self.val_steps.append(trainer.global_step)\n",
    "        \n",
    "metrics = MetricsCallback()\n",
    "trainer_kwargs = {\n",
    "    'accelerator': 'gpu',\n",
    "    'max_epochs': 20,\n",
    "    'log_every_n_steps': 2,\n",
    "    'callbacks': [metrics],\n",
    "    'enable_progress_bar': False,\n",
    "    'devices': 1\n",
    "}\n",
    "\n",
    "oracle = ClassifierFeatureMap(\n",
    "    configs.classes,\n",
    "    1e-2,\n",
    "    trainer_kwargs, \n",
    "    seed=configs.rng_seed\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\") #Ignore warnings about num_workers\n",
    "\n",
    "oracle.fit(train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the data\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots #Nicer styles\n",
    "plt.style.use(['science', 'retro'])\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(metrics.train_steps, metrics.train_acc, label='Train')\n",
    "ax.plot(metrics.val_steps, metrics.val_acc, label='Validation')\n",
    "ax.set_xlabel('Global step')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(frameon=False)\n",
    "ax.margins(x=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kooplearn.abc\n",
    "def evaluate_model(model: kooplearn.abc.BaseModel, test_data):\n",
    "    assert model.is_fitted\n",
    "    test_labels = test_data['label']\n",
    "    test_images = test_data['image']\n",
    "    test_images = np.expand_dims(test_images, 1)\n",
    "    report = {\n",
    "        'accuracy': [],\n",
    "        'label': [],\n",
    "        'image': [],\n",
    "        'times': []\n",
    "    }\n",
    "    for t in range(1, configs.eval_up_to_t + 1):\n",
    "        pred = model.predict(test_images, t=t).reshape(-1, 28 ,28)\n",
    "        pred_labels = oracle(pred)\n",
    "        pred_labels = pred_labels.argmax(axis=1)\n",
    "        accuracy = (pred_labels == (test_labels + t)%configs.classes ).mean()\n",
    "        report['accuracy'].append(accuracy)\n",
    "        report['image'].append(pred)\n",
    "        report['label'].append(pred_labels)\n",
    "        report['times'].append(t)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kooplearn.data import traj_to_contexts\n",
    "\n",
    "context_len = 2\n",
    "train_data = traj_to_contexts(np_ordered_MNIST['train']['image'], context_len)\n",
    "val_data = traj_to_contexts(np_ordered_MNIST['validation']['image'], context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data loaders\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ContextWindowDataset(Dataset):\n",
    "    #Minimal Torch dataset to handle context windows. TODO: Add to kooplearn\n",
    "    def __init__(self, contexts):\n",
    "        self.contexts = contexts\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx]\n",
    "\n",
    "train_ds = ContextWindowDataset(torch.from_numpy(train_data.copy()))\n",
    "val_ds = ContextWindowDataset(torch.from_numpy(val_data.copy()))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=configs.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from kooplearn.models.feature_maps.vampnets import VAMPNet\n",
    "from kooplearn.models import DeepEDMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:\n",
    "1. Defining the `lightning` trainer\n",
    "2. Tensorboard logging through callbacks\n",
    "3. `optuna` HP tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModel(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % 10 == 0:\n",
    "            fmap = pl_module._kooplearn_feature_map_weakref()\n",
    "            assert isinstance(fmap, VAMPNet)\n",
    "            fmap._is_fitted = True\n",
    "            koopman_model = DeepEDMD(\n",
    "                fmap, \n",
    "                reduced_rank=False,\n",
    "                rank = configs.classes).fit(train_data)\n",
    "            fmap._is_fitted = False\n",
    "            report = evaluate_model(koopman_model, np_ordered_MNIST['validation'])\n",
    "            pl_module.log('val/accuracy', np.mean(report['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "tb_logger = TensorBoardLogger('tb_logs', name='VAMPNet', default_hp_metric=False)\n",
    "#Defining the lightning trainer\n",
    "eval_cb = EvaluateModel()\n",
    "trainer_kwargs = {\n",
    "    'accelerator': 'gpu',\n",
    "    'devices': 1,\n",
    "    'max_epochs': configs.max_epochs,  \n",
    "    'log_every_n_steps': 3,\n",
    "    'logger': tb_logger,\n",
    "    'callbacks': [eval_cb],\n",
    "}\n",
    "trainer = lightning.Trainer(**trainer_kwargs)\n",
    "\n",
    "#Defining the model\n",
    "model = VAMPNet(\n",
    "    CNNEncoder,\n",
    "    torch.optim.Adam,\n",
    "    {'lr': 1e-4},\n",
    "    trainer,\n",
    "    {'num_classes': configs.classes},\n",
    "    center_covariances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | lobe | CNNEncoder | 21.1 K\n",
      "------------------------------------\n",
      "21.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 K    Total params\n",
      "0.084     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4957a8f7ec964928b9059c27dff14484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kooplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
